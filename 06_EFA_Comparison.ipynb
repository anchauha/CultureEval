{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bba8e7d",
   "metadata": {},
   "source": [
    "# WVS Wave 7 - EFA Comparison: Ground Truth vs. LLM Simulation\n",
    "\n",
    "This script compares a ground truth dataset (T1) with an LLM-simulated dataset (T2) using Exploratory Factor Analysis (EFA).\n",
    "\n",
    "### Workflow:\n",
    "1. Load and prepare both datasets, ensuring common variables and data types.\n",
    "2. Perform EFA on the ground truth data (T1) to establish the reference factor structure.\n",
    "3. (Optional) Perform EFA on the simulated data (T2) for congruence analysis.\n",
    "4. (Optional) Calculate Tucker's Congruence Coefficient between T1 and T2 factor loadings.\n",
    "5. Project the simulated data (T2) onto the T1 factor structure using the T1 FA model.\n",
    "6. Compare the original T1 factor scores with the projected T2 scores using t-tests and Cohen's d.\n",
    "7. Analyze differences in factor scores across demographic groups (country, region).\n",
    "8. (Optional) Analyze raw score changes for specific variables by group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab90b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Imports\n",
    "# =============================================================================\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import datetime as dt\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pathlib\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "from scipy.stats import ttest_rel # For paired t-tests\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='factor_analyzer')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Configuration / Constants\n",
    "# =============================================================================\n",
    "DATA_DIR = pathlib.Path(\"./data\")\n",
    "OUTPUT_DIR_ROOT = pathlib.Path(\"./output/efa_comparison_T1_vs_T2\")\n",
    "\n",
    "# --- Input Data ---\n",
    "# Ground Truth Aggregated Data (Result of Aggregation Script)\n",
    "T1_AGGREGATED_PATH = DATA_DIR / \"new_median_wvs_wave7_aggregated_by_demographics.csv\"\n",
    "# LLM Simulated Aggregated Data (Needs to be generated and have same structure)\n",
    "T2_AGGREGATED_PATH = DATA_DIR / \"phi4-14b_output_inferred.csv\" # <-- UPDATE THIS PATH\n",
    "# Variable information (Binary/Ordinal types)\n",
    "VARIABLE_INFO_PATH = DATA_DIR / \"variable_info.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9569cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EFA Parameters ---\n",
    "# Determined from previous EFA steps (e.g., comparison script)\n",
    "N_FACTORS = 5 # Number of factors identified for T1\n",
    "ROTATION = 'promax' # Rotation used for the reference T1 model\n",
    "FACTOR_METHOD = 'principal' # Extraction method used for T1\n",
    "USE_SMC = True # Use Squared Multiple Correlation for communalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "79f2f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comparison Parameters ---\n",
    "# For Tucker's Congruence analysis (optional)\n",
    "RUN_CONGRUENCE_ANALYSIS = True # Set to True to also run FA on T2 and compare\n",
    "CONGRUENCE_THRESHOLD_HIGH = 0.90 # Threshold for strong factor correspondence\n",
    "CONGRUENCE_THRESHOLD_LOW = 0.85 # Threshold for acceptable correspondence (use with caution)\n",
    "CONGRUENCE_OFF_DIAGONAL_MAX = 0.3 # Max acceptable loading on non-corresponding factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Group Change Analysis\n",
    "COUNTRY_COLUMN = 'B_COUNTRY_ALPHA' # Column containing country identifiers\n",
    "WESTERN_COUNTRIES = ['USA','CAN','GBR','FRA','DEU','NLD','AUS','NZL'] # Western List\n",
    "\n",
    "# For Raw Variable Change Analysis\n",
    "ANALYZE_RAW_VARIABLE = True # Set to True to analyze specific variable\n",
    "RAW_VARIABLE_TO_ANALYZE = 'Q165P' # Homosexuality Acceptance (Q165P)\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc629cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper Functions\n",
    "# =============================================================================\n",
    "\n",
    "def ensure_output_dir(root: pathlib.Path, *subdirs: str) -> pathlib.Path:\n",
    "    \"\"\"Creates nested output directories.\"\"\"\n",
    "    out_path = root.joinpath(*subdirs)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Output directory ensured: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "def log_message(msg: str, level: str = \"INFO\") -> None:\n",
    "    \"\"\"Prints a formatted message.\"\"\"\n",
    "    print(f\"[{level}] {msg}\", flush=True)\n",
    "\n",
    "def load_and_prepare_data(t1_agg_path: pathlib.Path, t2_agg_path: pathlib.Path,\n",
    "                          var_info_path: pathlib.Path) -> tuple[pd.DataFrame, pd.DataFrame, list, list, pd.DataFrame]:\n",
    "    \"\"\"Loads T1 and T2, identifies common variables, validates, returns prepared data.\"\"\"\n",
    "    log_message(\"--- Loading and Preparing Data (T1 & T2) ---\")\n",
    "\n",
    "    # --- Load Data ---\n",
    "    if not t1_agg_path.exists(): raise FileNotFoundError(f\"T1 file not found: {t1_agg_path}\")\n",
    "    if not t2_agg_path.exists(): raise FileNotFoundError(f\"T2 file not found: {t2_agg_path}\")\n",
    "    if not var_info_path.exists(): raise FileNotFoundError(f\"Variable info file not found: {var_info_path}\")\n",
    "\n",
    "    try:\n",
    "        data_T1_raw = pd.read_csv(t1_agg_path)\n",
    "        data_T2_raw = pd.read_csv(t2_agg_path)\n",
    "        var_info = pd.read_csv(var_info_path)\n",
    "        log_message(f\"Loaded T1: {data_T1_raw.shape}, T2: {data_T2_raw.shape}, VarInfo: {var_info.shape}\")\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading input files: {e}\")\n",
    "\n",
    "    # --- Identify Variable Types and Common Columns ---\n",
    "    if not all(c in var_info.columns for c in ['Variable_Code', 'Type']):\n",
    "        raise ValueError(\"Variable info file must contain 'Variable_Code' and 'Type' columns.\")\n",
    "    binary_vars = var_info.loc[var_info['Type'].str.lower() == 'binary', 'Variable_Code'].tolist()\n",
    "    ordinal_vars = var_info.loc[var_info['Type'].str.lower() == 'ordinal', 'Variable_Code'].tolist()\n",
    "\n",
    "    common_binary = sorted(list(set(binary_vars) & set(data_T1_raw.columns) & set(data_T2_raw.columns)))\n",
    "    common_ordinal = sorted(list(set(ordinal_vars) & set(data_T1_raw.columns) & set(data_T2_raw.columns)))\n",
    "    common_survey_vars = common_binary + common_ordinal\n",
    "\n",
    "    if not common_survey_vars:\n",
    "        raise ValueError(\"No common Binary or Ordinal survey variables found between T1, T2, and variable info.\")\n",
    "    log_message(f\"Found {len(common_binary)} common Binary and {len(common_ordinal)} common Ordinal variables.\")\n",
    "\n",
    "    # --- Subset and Prepare Data ---\n",
    "    # Keep only common survey vars + necessary demographic/ID columns for T1 (used later)\n",
    "    t1_cols_to_keep = common_survey_vars + ([COUNTRY_COLUMN] if COUNTRY_COLUMN in data_T1_raw.columns else [])\n",
    "    data_T1 = data_T1_raw[t1_cols_to_keep].copy()\n",
    "    # T2 only needs the common survey variables for projection\n",
    "    data_T2 = data_T2_raw[common_survey_vars].copy()\n",
    "\n",
    "    # Assumes the *meaning* of rows corresponds (same demographic profile)\n",
    "    # If indices are meaningful and match, use reindex:\n",
    "\n",
    "    if data_T1.index.equals(data_T2.index):\n",
    "       log_message(\"Indices match between T1 and T2.\")\n",
    "    else:\n",
    "       log_message(\"Warning: Indices do not match. Assuming row order corresponds for comparison.\")\n",
    "       data_T2 = data_T2.reindex(data_T1.index) # HARD LESSON: Requires meaningful, aligned index in CSVs\n",
    "\n",
    "    # Coerce to appropriate types (numeric for FA)\n",
    "    for df, name in [(data_T1, \"T1\"), (data_T2, \"T2\")]:\n",
    "        for col in common_survey_vars:\n",
    "            try:\n",
    "                # Attempt conversion to numeric, coerce errors to NaN\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            except Exception as e:\n",
    "                log_message(f\"Warning: Could not convert column '{col}' in {name} to numeric: {e}\", \"WARN\")\n",
    "        # Convert to integer (only if no NaNs introduced)\n",
    "        df[common_survey_vars] = df[common_survey_vars].astype(int)\n",
    "\n",
    "    # --- Validation ---\n",
    "    for df, name in [(data_T1[common_survey_vars], \"T1_survey\"), (data_T2, \"T2_survey\")]:\n",
    "        if df.isnull().any().any():\n",
    "             nan_cols = df.columns[df.isnull().any()].tolist()\n",
    "             raise ValueError(f\"NaNs present in {name} after preparation (Columns: {nan_cols}). Impute or check data source.\")\n",
    "        if not all(pd.api.types.is_numeric_dtype(df[col]) for col in df.columns):\n",
    "             non_num_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]\n",
    "             raise TypeError(f\"Non-numeric data detected in {name} columns: {non_num_cols}\")\n",
    "        zero_var_cols = df.columns[df.var() == 0].tolist()\n",
    "        if zero_var_cols:\n",
    "             raise ValueError(f\"Zero variance columns found in {name}: {zero_var_cols}\")\n",
    "\n",
    "    log_message(\"Data loading and preparation complete.\")\n",
    "\n",
    "    # Return T1 (with demographics), T2 (survey only), common survey vars, common binary, common ordinal\n",
    "    return data_T1, data_T2, common_survey_vars, common_binary, common_ordinal\n",
    "\n",
    "\n",
    "def run_factor_analysis(data: pd.DataFrame, n_factors: int, rotation: str | None, method: str,\n",
    "                        use_smc: bool, out_dir: pathlib.Path, label: str) -> tuple[FactorAnalyzer, dict]:\n",
    "    \"\"\"Performs EFA and saves results.\"\"\"\n",
    "    log_message(f\"--- Running Factor Analysis on {label} ---\")\n",
    "    results = {}\n",
    "\n",
    "    # Define output paths\n",
    "    kmo_file = out_dir / f'kmo_per_variable_{label}.csv'\n",
    "    pattern_file = out_dir / f'loadings_{rotation}_{n_factors}factors_{label}.csv'\n",
    "    structure_file = out_dir / f'structure_{rotation}_{n_factors}factors_{label}.csv'\n",
    "    corr_file = out_dir / f'factor_corr_{rotation}_{n_factors}factors_{label}.csv'\n",
    "    var_file = out_dir / f'variance_{rotation}_{n_factors}factors_{label}.csv'\n",
    "    comm_file = out_dir / f'communalities_{rotation}_{n_factors}factors_{label}.csv'\n",
    "    scores_file = out_dir / f'scores_{rotation}_{n_factors}factors_{label}.csv'\n",
    "\n",
    "    # --- Suitability Tests ---\n",
    "    try:\n",
    "        chi2, p = calculate_bartlett_sphericity(data)\n",
    "        log_message(f\" Bartlett's Test: Chi2={chi2:.2f}, p={p:.4g}\")\n",
    "        if p >= 0.05: log_message(f\"Bartlett p-value >= 0.05, suitability questionable.\", \"WARN\")\n",
    "        km_per_var, km_overall = calculate_kmo(data)\n",
    "        log_message(f\" KMO Test: Overall={km_overall:.3f}\")\n",
    "        if km_overall < 0.6: log_message(f\"Overall KMO < 0.6, suitability questionable.\", \"WARN\")\n",
    "        pd.DataFrame({'KMO': km_per_var}, index=data.columns).sort_values('KMO').to_csv(kmo_file)\n",
    "    except Exception as e:\n",
    "        log_message(f\"Could not perform suitability tests for {label}: {e}\", \"ERROR\")\n",
    "\n",
    "    # --- Fit FA Model ---\n",
    "    try:\n",
    "        fa = FactorAnalyzer(n_factors=n_factors, rotation=rotation, method=method,\n",
    "                            use_smc=use_smc, rotation_kwargs={'max_iter': 1000})\n",
    "        fa.fit(data)\n",
    "        log_message(\" EFA model fitted successfully.\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"EFA model fitting failed for {label}: {e}\", \"ERROR\")\n",
    "        raise e # Stop execution if fitting fails\n",
    "\n",
    "    # --- Extract and Save Results ---\n",
    "    factor_names = [f'Factor_{i+1}' for i in range(n_factors)]\n",
    "    results['factor_names'] = factor_names\n",
    "\n",
    "    try:\n",
    "        # Loadings (Pattern matrix for oblique, Factor matrix for orthogonal)\n",
    "        results['loadings'] = pd.DataFrame(fa.loadings_, index=data.columns, columns=factor_names)\n",
    "        results['loadings'].to_csv(pattern_file)\n",
    "\n",
    "        # Communalities\n",
    "        results['communalities'] = pd.DataFrame({'Communality': fa.get_communalities()}, index=data.columns)\n",
    "        results['communalities'].to_csv(comm_file)\n",
    "\n",
    "        # Variance Explained\n",
    "        var, prop, cum = fa.get_factor_variance()\n",
    "        results['variance'] = pd.DataFrame({'Factor': factor_names, 'SSL': var, 'Prop_Var': prop, 'Cum_Var': cum})\n",
    "        results['variance'].to_csv(var_file, index=False)\n",
    "\n",
    "        # Factor Correlations (Phi) - only for oblique rotations\n",
    "        if fa.phi_ is not None:\n",
    "            results['factor_correlations'] = pd.DataFrame(fa.phi_, index=factor_names, columns=factor_names)\n",
    "            results['factor_correlations'].to_csv(corr_file)\n",
    "\n",
    "        # Structure Matrix - only for oblique rotations\n",
    "        if hasattr(fa, 'structure_') and fa.structure_ is not None:\n",
    "            results['structure'] = pd.DataFrame(fa.structure_, index=data.columns, columns=factor_names)\n",
    "            results['structure'].to_csv(structure_file)\n",
    "\n",
    "        # Factor Scores (calculate and save)\n",
    "        results['scores'] = pd.DataFrame(fa.transform(data), index=data.index, columns=factor_names)\n",
    "        results['scores'].to_csv(scores_file)\n",
    "\n",
    "        log_message(f\" EFA results saved to: {out_dir}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error extracting or saving results for {label}: {e}\", \"ERROR\")\n",
    "        # Decide if partial results are acceptable or raise error\n",
    "\n",
    "    return fa, results\n",
    "\n",
    "\n",
    "def calculate_congruence(loadings1: pd.DataFrame, loadings2: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculates Tucker's Congruence Coefficient matrix between two loading matrices.\"\"\"\n",
    "    log_message(\"--- Calculating Tucker's Congruence Coefficients ---\")\n",
    "    F1 = loadings1.values\n",
    "    F2 = loadings2.values\n",
    "\n",
    "    if F1.shape[0] != F2.shape[0]:\n",
    "        log_message(\"Row mismatch between loading matrices. Attempting to reindex T2 based on T1.\", \"WARN\")\n",
    "        common_index = loadings1.index.intersection(loadings2.index)\n",
    "        if len(common_index) < F1.shape[0] * 0.8: # Heuristic check\n",
    "             raise ValueError(\"Significant row mismatch, cannot reliably calculate congruence.\")\n",
    "        F1 = loadings1.loc[common_index].values\n",
    "        F2 = loadings2.loc[common_index].values\n",
    "        log_message(f\" Congruence calculated on {len(common_index)} common variables.\")\n",
    "\n",
    "    n_f1, n_f2 = F1.shape[1], F2.shape[1]\n",
    "    congruence_matrix = np.zeros((n_f1, n_f2))\n",
    "\n",
    "    for i in range(n_f1):\n",
    "        for j in range(n_f2):\n",
    "            numerator = np.sum(F1[:, i] * F2[:, j])\n",
    "            denominator = np.sqrt(np.sum(F1[:, i]**2) * np.sum(F2[:, j]**2))\n",
    "            congruence_matrix[i, j] = numerator / denominator if denominator != 0 else np.nan\n",
    "\n",
    "    congruence_df = pd.DataFrame(congruence_matrix, index=loadings1.columns, columns=loadings2.columns)\n",
    "    log_message(\"Congruence calculation complete.\")\n",
    "    return congruence_df\n",
    "\n",
    "\n",
    "def analyze_congruence(congruence_df: pd.DataFrame, high_threshold: float,\n",
    "                       low_threshold: float, off_diag_max: float, out_dir: pathlib.Path) -> None:\n",
    "    \"\"\"Analyzes and interprets the congruence coefficient matrix.\"\"\"\n",
    "    log_message(\"--- Analyzing Congruence ---\")\n",
    "    print(\"Congruence Matrix (T1 vs T2):\")\n",
    "    print(congruence_df.round(3))\n",
    "\n",
    "    # Best match for each T1 factor\n",
    "    best_match_df = pd.DataFrame({\n",
    "        'Best_Matching_T2_Factor': congruence_df.idxmax(axis=1),\n",
    "        'Congruence_Coefficient': congruence_df.max(axis=1)\n",
    "    })\n",
    "    print(\"\\nBest T2 Match for each T1 Factor:\")\n",
    "    print(best_match_df.round(3))\n",
    "    best_match_df.to_csv(out_dir / \"congruence_best_matches.csv\")\n",
    "\n",
    "    # Check diagonal congruence and off-diagonal noise\n",
    "    if congruence_df.shape[0] == congruence_df.shape[1]:\n",
    "        diagonal_values = np.diag(congruence_df.values)\n",
    "        off_diagonal_mask = ~np.eye(congruence_df.shape[0], dtype=bool)\n",
    "        off_diagonal_values = np.abs(congruence_df.values[off_diagonal_mask])\n",
    "\n",
    "        num_high_congruence = np.sum(diagonal_values >= high_threshold)\n",
    "        num_low_congruence = np.sum(diagonal_values < low_threshold)\n",
    "        max_off_diagonal = off_diagonal_values.max() if off_diagonal_values.size > 0 else 0\n",
    "\n",
    "        log_message(f\"Diagonal congruence check: {num_high_congruence} factors >= {high_threshold:.2f}, {num_low_congruence} factors < {low_threshold:.2f}\")\n",
    "        log_message(f\"Off-diagonal check: Max absolute value = {max_off_diagonal:.3f} (Threshold < {off_diag_max:.2f})\")\n",
    "\n",
    "        if num_high_congruence == congruence_df.shape[0] and max_off_diagonal < off_diag_max:\n",
    "            log_message(\"Factor structure shows high congruence and good distinction.\")\n",
    "        elif num_low_congruence > 0 or max_off_diagonal >= off_diag_max:\n",
    "            log_message(\"Factor structure congruence is questionable. Check coefficients.\", \"WARN\")\n",
    "        else:\n",
    "            log_message(\"Factor structure shows moderate congruence.\")\n",
    "    else:\n",
    "        log_message(\"Congruence matrix is not square, skipping diagonal/off-diagonal checks.\")\n",
    "\n",
    "\n",
    "def project_and_compare_scores(fa_T1: FactorAnalyzer, data_T1_scores: pd.DataFrame,\n",
    "                               data_T2: pd.DataFrame, out_dir: pathlib.Path) -> pd.DataFrame:\n",
    "    \"\"\"Projects T2 data onto T1 factors, compares scores, calculates stats.\"\"\"\n",
    "    log_message(\"--- Projecting T2 onto T1 Factors and Comparing Scores ---\")\n",
    "\n",
    "    # Project T2 data using the T1 factor analysis object\n",
    "    try:\n",
    "        scores_proj_T2 = fa_T1.transform(data_T2) # data_T2 should only contain common survey vars\n",
    "        scores_proj_T2_df = pd.DataFrame(scores_proj_T2, index=data_T2.index, columns=data_T1_scores.columns)\n",
    "        scores_proj_T2_df.to_csv(out_dir / 'scores_T2_projected_on_T1.csv')\n",
    "        log_message(\" T2 data projected onto T1 factors.\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error projecting T2 data: {e}\", \"ERROR\")\n",
    "        raise e\n",
    "\n",
    "    # --- Paired t-tests: T1 scores vs Projected T2 scores ---\n",
    "    log_message(\" Running paired t-tests (T1 vs Projected T2)...\")\n",
    "    ttest_results = []\n",
    "    common_index = data_T1_scores.index.intersection(scores_proj_T2_df.index)\n",
    "    if len(common_index) < len(data_T1_scores) or len(common_index) < len(scores_proj_T2_df):\n",
    "         log_message(f\"Warning: Comparing scores on {len(common_index)} common indices only.\", \"WARN\")\n",
    "\n",
    "    t1_scores_common = data_T1_scores.loc[common_index]\n",
    "    t2_proj_scores_common = scores_proj_T2_df.loc[common_index]\n",
    "\n",
    "    for factor in t1_scores_common.columns:\n",
    "        try:\n",
    "            t_stat, p_value = ttest_rel(t1_scores_common[factor], t2_proj_scores_common[factor])\n",
    "            ttest_results.append({'Factor': factor, 'T_Statistic': t_stat, 'P_Value': p_value})\n",
    "        except Exception as e:\n",
    "            log_message(f\"Could not perform t-test for {factor}: {e}\", \"WARN\")\n",
    "            ttest_results.append({'Factor': factor, 'T_Statistic': np.nan, 'P_Value': np.nan})\n",
    "\n",
    "    ttest_df = pd.DataFrame(ttest_results).set_index('Factor')\n",
    "    ttest_df.to_csv(out_dir / 'ttest_T1_vs_T2projected.csv')\n",
    "    print(\" Paired t-test results (T1 vs Projected T2):\")\n",
    "    print(ttest_df.round(4))\n",
    "\n",
    "    # --- Cohen's d for effect size ---\n",
    "    log_message(\" Calculating Cohen's d for score differences...\")\n",
    "    cohen_d_results = []\n",
    "    # Calculate difference based on common indices\n",
    "    diff_scores = t2_proj_scores_common.subtract(t1_scores_common)\n",
    "    for factor in diff_scores.columns:\n",
    "        mean_diff = diff_scores[factor].mean()\n",
    "        std_diff = diff_scores[factor].std(ddof=1) # Use sample standard deviation\n",
    "        cohen_d = mean_diff / std_diff if std_diff != 0 else 0\n",
    "        cohen_d_results.append({'Factor': factor, \"Mean_Difference\": mean_diff, \"Std_Dev_Difference\": std_diff, \"Cohen_d\": cohen_d})\n",
    "        # print(f\"  {factor}: Mean Diff={mean_diff:.3f}, SD Diff={std_diff:.3f}, Cohen's d = {cohen_d:.3f}\")\n",
    "\n",
    "    cohen_d_df = pd.DataFrame(cohen_d_results).set_index('Factor')\n",
    "    cohen_d_df.to_csv(out_dir / 'cohens_d_T1_vs_T2projected.csv')\n",
    "    print(\" Cohen's d results:\")\n",
    "    print(cohen_d_df.round(4))\n",
    "\n",
    "    log_message(\" Score comparison complete.\")\n",
    "    return diff_scores # Return differences on common index\n",
    "\n",
    "\n",
    "def analyze_group_changes(diff_scores: pd.DataFrame, t1_aggregated_data: pd.DataFrame,\n",
    "                          country_col: str, region_map: dict, out_dir: pathlib.Path) -> None:\n",
    "    \"\"\"Analyzes mean factor score differences by country and region.\"\"\"\n",
    "    log_message(\"--- Analyzing Group Changes in Factor Scores ---\")\n",
    "\n",
    "    # Merge demographic info (country) using the index\n",
    "    if country_col not in t1_aggregated_data.columns:\n",
    "        log_message(f\"Country column '{country_col}' not found in T1 aggregated data. Skipping group analysis.\", \"WARN\")\n",
    "        return\n",
    "\n",
    "    # Select only country column and ensure index matches diff_scores\n",
    "    demo_info = t1_aggregated_data[[country_col]].copy()\n",
    "    # Align indices before merging - crucial step\n",
    "    common_index = diff_scores.index.intersection(demo_info.index)\n",
    "    if len(common_index) < len(diff_scores):\n",
    "         log_message(f\"Group analysis performed on {len(common_index)} participants with matching demographic info.\", \"WARN\")\n",
    "\n",
    "    diff_scores_grp = diff_scores.loc[common_index].copy()\n",
    "    demo_info = demo_info.loc[common_index]\n",
    "    diff_scores_grp[country_col] = demo_info[country_col]\n",
    "\n",
    "\n",
    "    # --- Mean Change by Country ---\n",
    "    try:\n",
    "        # Ensure only numeric factor columns are aggregated\n",
    "        numeric_factor_cols = diff_scores_grp.select_dtypes(include=np.number).columns.tolist()\n",
    "        country_mean_change = diff_scores_grp.groupby(country_col)[numeric_factor_cols].mean()\n",
    "        country_mean_change.to_csv(out_dir / 'country_mean_factor_change.csv')\n",
    "        log_message(\" Calculated mean factor change by country.\")\n",
    "        print(\"\\n Mean Factor Score Change by Country (Projected T2 - T1):\")\n",
    "        print(country_mean_change.round(4).head()) # Print head\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error calculating country mean change: {e}\", \"ERROR\")\n",
    "\n",
    "    # --- Mean Change by Region ---\n",
    "    try:\n",
    "        diff_scores_grp['region'] = diff_scores_grp[country_col].apply(\n",
    "            lambda x: region_map.get(x, 'Other') # Use map, default to 'Other'\n",
    "        )\n",
    "        numeric_factor_cols = diff_scores_grp.select_dtypes(include=np.number).columns.tolist()\n",
    "        region_mean_change = diff_scores_grp.groupby('region')[numeric_factor_cols].mean()\n",
    "        region_mean_change.to_csv(out_dir / 'region_mean_factor_change.csv')\n",
    "        log_message(\" Calculated mean factor change by region.\")\n",
    "        print(\"\\n Mean Factor Score Change by Region (Projected T2 - T1):\")\n",
    "        print(region_mean_change.round(4))\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error calculating region mean change: {e}\", \"ERROR\")\n",
    "\n",
    "    log_message(\"Group change analysis complete.\")\n",
    "\n",
    "\n",
    "def analyze_raw_variable_change(t1_agg_path: pathlib.Path, t2_agg_path: pathlib.Path,\n",
    "                                variable: str, country_col: str, out_dir: pathlib.Path) -> None:\n",
    "    \"\"\"Analyzes the change in a specific raw variable score by country.\"\"\"\n",
    "    log_message(f\"--- Analyzing Raw Change for Variable: {variable} ---\")\n",
    "\n",
    "    try:\n",
    "        # Reload necessary columns only to minimize memory usage\n",
    "        cols_to_load = [country_col, variable]\n",
    "        # Need index if participant_id is index, otherwise need participant_id col\n",
    "        use_cols_t1 = [col for col in cols_to_load if col in pd.read_csv(t1_agg_path, nrows=0).columns]\n",
    "        use_cols_t2 = [col for col in cols_to_load if col in pd.read_csv(t2_agg_path, nrows=0).columns]\n",
    "\n",
    "        # Assume first column is index if not participant_id\n",
    "        # So assume index_col=0 is standard if participant_id isn't used. Adjust if needed.\n",
    "        index_col_name = 'participant_id' # ASSUME this is the index name in CSV\n",
    "        if index_col_name not in pd.read_csv(t1_agg_path, nrows=0).columns:\n",
    "             index_col_name = None # Use default index 0 if not present\n",
    "\n",
    "        t1_raw = pd.read_csv(t1_agg_path, usecols=use_cols_t1 + ([index_col_name] if index_col_name else []), index_col=index_col_name)\n",
    "        t2_raw = pd.read_csv(t2_agg_path, usecols=use_cols_t2 + ([index_col_name] if index_col_name else []), index_col=index_col_name)\n",
    "\n",
    "\n",
    "        if variable not in t1_raw.columns or variable not in t2_raw.columns:\n",
    "            log_message(f\"Variable '{variable}' not found in both T1 and T2 raw files. Skipping.\", \"WARN\")\n",
    "            return\n",
    "        if country_col not in t1_raw.columns:\n",
    "            log_message(f\"Country column '{country_col}' not found in T1 raw file. Skipping.\", \"WARN\")\n",
    "            return\n",
    "\n",
    "        # Align data on common index\n",
    "        common_index = t1_raw.index.intersection(t2_raw.index)\n",
    "        t1_aligned = t1_raw.loc[common_index]\n",
    "        t2_aligned = t2_raw.loc[common_index]\n",
    "\n",
    "        # Calculate difference\n",
    "        var_diff = (t2_aligned[variable] - t1_aligned[variable]).to_frame(f'{variable}_diff')\n",
    "        var_diff[country_col] = t1_aligned[country_col] # Add country from aligned T1\n",
    "\n",
    "        # Group by country\n",
    "        country_var_change = var_diff.groupby(country_col)[f'{variable}_diff'].mean()\n",
    "        country_var_change.to_csv(out_dir / f'country_{variable}_raw_change.csv')\n",
    "        log_message(f\" Calculated mean raw change for {variable} by country.\")\n",
    "        print(f\"\\n Mean Raw Score Change for {variable} by Country (T2 - T1):\")\n",
    "        print(country_var_change.reindex(country_var_change.abs().sort_values(ascending=False).index).round(4))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        log_message(f\"Could not load raw data files for variable '{variable}' change analysis.\", \"ERROR\")\n",
    "    except KeyError as e:\n",
    "        log_message(f\"Column missing for raw variable change analysis: {e}\", \"ERROR\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during raw variable change analysis for '{variable}': {e}\", \"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d49750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # Main Execution Workflow\n",
    "# # =============================================================================\n",
    "\n",
    "# Ignoring for now\n",
    "# def main(args):\n",
    "#     \"\"\"Main function to orchestrate the EFA comparison pipeline.\"\"\"\n",
    "#     log_message(\"--- Starting EFA Comparison Pipeline (T1 vs T2) ---\")\n",
    "\n",
    "#     # --- Setup Output Directories ---\n",
    "#     comp_out_dir = ensure_output_dir(pathlib.Path(args.output_root)) # Main comparison output\n",
    "#     t1_out_dir = ensure_output_dir(pathlib.Path(args.output_root), \"T1_EFA_results\")\n",
    "#     t2_out_dir = ensure_output_dir(pathlib.Path(args.output_root), \"T2_EFA_results\") if args.run_congruence else None\n",
    "\n",
    "#     # --- Load and Prepare Data ---\n",
    "#     # Pass paths directly from args\n",
    "#     t1_path = pathlib.Path(args.t1_file)\n",
    "#     t2_path = pathlib.Path(args.t2_file)\n",
    "#     var_info_path = pathlib.Path(args.var_file)\n",
    "#     df_T1_full, df_T2_survey, common_vars, _, _ = load_and_prepare_data(\n",
    "#         t1_path, t2_path, var_info_path\n",
    "#     )\n",
    "#     df_T1_survey = df_T1_full[common_vars] # Extract survey vars for T1 EFA\n",
    "\n",
    "#     # --- Run EFA on T1 (Ground Truth) ---\n",
    "#     fa_T1, results_T1 = run_factor_analysis(\n",
    "#         data=df_T1_survey,\n",
    "#         n_factors=args.n_factors,\n",
    "#         rotation=args.rotation,\n",
    "#         method=args.method,\n",
    "#         use_smc=args.use_smc,\n",
    "#         out_dir=t1_out_dir,\n",
    "#         label=\"T1\"\n",
    "#     )\n",
    "#     # Extract T1 scores for later comparison\n",
    "#     data_T1_scores = results_T1.get('scores')\n",
    "#     if data_T1_scores is None:\n",
    "#          log_message(\"Could not retrieve T1 factor scores. Aborting comparison.\", \"ERROR\")\n",
    "#          sys.exit(1)\n",
    "\n",
    "\n",
    "#     # --- (Optional) Run EFA on T2 and Calculate Congruence ---\n",
    "#     if args.run_congruence:\n",
    "#         if t2_out_dir:\n",
    "#              fa_T2, results_T2 = run_factor_analysis(\n",
    "#                  data=df_T2_survey, # Use only common survey vars\n",
    "#                  n_factors=args.n_factors,\n",
    "#                  rotation=args.rotation,\n",
    "#                  method=args.method,\n",
    "#                  use_smc=args.use_smc,\n",
    "#                  out_dir=t2_out_dir,\n",
    "#                  label=\"T2\"\n",
    "#              )\n",
    "#              # Calculate and analyze congruence\n",
    "#              if 'loadings' in results_T1 and 'loadings' in results_T2:\n",
    "#                   congruence_df = calculate_congruence(results_T1['loadings'], results_T2['loadings'])\n",
    "#                   analyze_congruence(congruence_df, args.cong_high, args.cong_low, args.cong_off_diag, comp_out_dir)\n",
    "#              else:\n",
    "#                   log_message(\"Could not calculate congruence, missing loading matrices.\", \"WARN\")\n",
    "#         else:\n",
    "#              log_message(\"T2 output directory not created, skipping T2 EFA run.\", \"WARN\")\n",
    "\n",
    "\n",
    "#     # --- Project T2 onto T1 and Compare Scores ---\n",
    "#     diff_scores = project_and_compare_scores(\n",
    "#         fa_T1=fa_T1,\n",
    "#         data_T1_scores=data_T1_scores,\n",
    "#         data_T2=df_T2_survey, # Use only common survey vars\n",
    "#         out_dir=comp_out_dir\n",
    "#     )\n",
    "\n",
    "#     # --- Analyze Group Changes ---\n",
    "#     # Create region map based on config\n",
    "#     region_map = {country: 'Western' if country in WESTERN_COUNTRIES else 'Non-Western'\n",
    "#                   for country in df_T1_full[COUNTRY_COLUMN].unique()}\n",
    "#     analyze_group_changes(\n",
    "#         diff_scores=diff_scores,\n",
    "#         t1_aggregated_data=df_T1_full, # Pass full T1 with demographics\n",
    "#         country_col=args.country_col,\n",
    "#         region_map=region_map,\n",
    "#         out_dir=comp_out_dir\n",
    "#     )\n",
    "\n",
    "#     # --- (Optional) Analyze Raw Variable Change ---\n",
    "#     if args.analyze_raw:\n",
    "#         analyze_raw_variable_change(\n",
    "#             t1_agg_path=t1_path,\n",
    "#             t2_agg_path=t2_path,\n",
    "#             variable=args.raw_var,\n",
    "#             country_col=args.country_col,\n",
    "#             out_dir=comp_out_dir\n",
    "#         )\n",
    "\n",
    "#     log_message(\"--- EFA Comparison Pipeline Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4cc66943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # Entry Point & Argument Parsing\n",
    "# # =============================================================================\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description=\"Compare Ground Truth (T1) vs LLM Simulated (T2) data using EFA.\")\n",
    "\n",
    "#     # Input Files\n",
    "#     parser.add_argument(\"--t1_file\", default=str(T1_AGGREGATED_PATH), help=\"Path to T1 (ground truth) aggregated CSV.\")\n",
    "#     parser.add_argument(\"--t2_file\", default=str(T2_AGGREGATED_PATH), help=\"Path to T2 (LLM simulated) aggregated CSV.\")\n",
    "#     parser.add_argument(\"--var_file\", default=str(VARIABLE_INFO_PATH), help=\"Path to variable info CSV.\")\n",
    "\n",
    "#     # EFA Parameters (should match the final chosen model for T1)\n",
    "#     parser.add_argument(\"--n_factors\", type=int, default=N_FACTORS, help=\"Number of factors.\")\n",
    "#     parser.add_argument(\"--rotation\", default=ROTATION, help=\"Rotation method (e.g., 'promax', 'varimax', None).\")\n",
    "#     parser.add_argument(\"--method\", default=FACTOR_METHOD, help=\"Factor extraction method (e.g., 'principal', 'ml', 'minres').\")\n",
    "#     parser.add_argument(\"--use_smc\", type=bool, default=USE_SMC, help=\"Use Squared Multiple Correlation for communalities.\")\n",
    "\n",
    "#     # Comparison Options\n",
    "#     parser.add_argument(\"--run_congruence\", action='store_true', default=RUN_CONGRUENCE_ANALYSIS, help=\"Run FA on T2 and calculate Tucker's Congruence.\")\n",
    "#     parser.add_argument(\"--cong_high\", type=float, default=CONGRUENCE_THRESHOLD_HIGH, help=\"High threshold for congruence.\")\n",
    "#     parser.add_argument(\"--cong_low\", type=float, default=CONGRUENCE_THRESHOLD_LOW, help=\"Low threshold for congruence.\")\n",
    "#     parser.add_argument(\"--cong_off_diag\", type=float, default=CONGRUENCE_OFF_DIAGONAL_MAX, help=\"Max acceptable off-diagonal congruence.\")\n",
    "\n",
    "#     # Group Analysis Options\n",
    "#     parser.add_argument(\"--country_col\", default=COUNTRY_COLUMN, help=\"Column name for country identifier.\")\n",
    "\n",
    "#     # Raw Variable Analysis Options\n",
    "#     parser.add_argument(\"--analyze_raw\", action='store_true', default=ANALYZE_RAW_VARIABLE, help=\"Analyze raw change for a specific variable.\")\n",
    "#     parser.add_argument(\"--raw_var\", default=RAW_VARIABLE_TO_ANALYZE, help=\"Variable code for raw change analysis.\")\n",
    "\n",
    "#     # Output Options\n",
    "#     parser.add_argument(\"--output_root\", default=str(OUTPUT_DIR_ROOT), help=\"Root directory for all output.\")\n",
    "\n",
    "#     cli_args = parser.parse_args()\n",
    "\n",
    "#     # --- Execute Main Function ---\n",
    "#     try:\n",
    "#         main(cli_args)\n",
    "#     except FileNotFoundError as e:\n",
    "#         log_message(f\"Error: Input file not found. {e}\", \"CRITICAL\")\n",
    "#         sys.exit(1)\n",
    "#     except (ValueError, TypeError, KeyError) as e:\n",
    "#         log_message(f\"Error: Data validation or configuration issue. {e}\", \"CRITICAL\")\n",
    "#         sys.exit(1)\n",
    "#     except Exception as e:\n",
    "#         log_message(f\"An unexpected error occurred: {e}\", \"CRITICAL\")\n",
    "#         # import traceback # Uncomment for detailed traceback\n",
    "#         # traceback.print_exc()\n",
    "#         sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca49b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Main Execution Workflow\n",
    "# =============================================================================\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Main function to orchestrate the EFA comparison pipeline.\"\"\"\n",
    "    log_message(\"--- Starting EFA Comparison Pipeline (T1 vs T2) ---\")\n",
    "    \n",
    "    # --- Setup Output Directories ---\n",
    "    comp_out_dir = ensure_output_dir(pathlib.Path(args.output_root)) # Main comparison output\n",
    "    t1_out_dir = ensure_output_dir(pathlib.Path(args.output_root), \"T1_EFA_results\")\n",
    "    t2_out_dir = ensure_output_dir(pathlib.Path(args.output_root), \"T2_EFA_results\") if args.run_congruence else None\n",
    "    \n",
    "    # --- Load and Prepare Data ---\n",
    "    # Pass paths directly from args\n",
    "    t1_path = pathlib.Path(args.t1_file)\n",
    "    t2_path = pathlib.Path(args.t2_file)\n",
    "    var_info_path = pathlib.Path(args.var_file)\n",
    "    df_T1_full, df_T2_survey, common_vars, _, _ = load_and_prepare_data(\n",
    "        t1_path, t2_path, var_info_path\n",
    "    )\n",
    "    df_T1_survey = df_T1_full[common_vars] # Extract survey vars for T1 EFA\n",
    "    \n",
    "    # --- Run EFA on T1 (Ground Truth) ---\n",
    "    fa_T1, results_T1 = run_factor_analysis(\n",
    "        data=df_T1_survey,\n",
    "        n_factors=args.n_factors,\n",
    "        rotation=args.rotation,\n",
    "        method=args.method,\n",
    "        use_smc=args.use_smc,\n",
    "        out_dir=t1_out_dir,\n",
    "        label=\"T1\"\n",
    "    )\n",
    "    # Extract T1 scores for later comparison\n",
    "    data_T1_scores = results_T1.get('scores')\n",
    "    if data_T1_scores is None:\n",
    "         log_message(\"Could not retrieve T1 factor scores. Aborting comparison.\", \"ERROR\")\n",
    "         sys.exit(1)\n",
    "    \n",
    "    \n",
    "    # --- Run EFA on T2 and Calculate Congruence ---\n",
    "    if args.run_congruence:\n",
    "        if t2_out_dir:\n",
    "             fa_T2, results_T2 = run_factor_analysis(\n",
    "                 data=df_T2_survey, # Use only common survey vars\n",
    "                 n_factors=args.n_factors,\n",
    "                 rotation=args.rotation,\n",
    "                 method=args.method,\n",
    "                 use_smc=args.use_smc,\n",
    "                 out_dir=t2_out_dir,\n",
    "                 label=\"T2\"\n",
    "             )\n",
    "             # Calculate and analyze congruence\n",
    "             if 'loadings' in results_T1 and 'loadings' in results_T2:\n",
    "                  congruence_df = calculate_congruence(results_T1['loadings'], results_T2['loadings'])\n",
    "                  analyze_congruence(congruence_df, args.cong_high, args.cong_low, args.cong_off_diag, comp_out_dir)\n",
    "             else:\n",
    "                  log_message(\"Could not calculate congruence, missing loading matrices.\", \"WARN\")\n",
    "        else:\n",
    "             log_message(\"T2 output directory not created, skipping T2 EFA run.\", \"WARN\")\n",
    "    \n",
    "    \n",
    "    # --- Project T2 onto T1 and Compare Scores ---\n",
    "    diff_scores = project_and_compare_scores(\n",
    "        fa_T1=fa_T1,\n",
    "        data_T1_scores=data_T1_scores,\n",
    "        data_T2=df_T2_survey, # Use only common survey vars\n",
    "        out_dir=comp_out_dir\n",
    "    )\n",
    "    \n",
    "    # --- Analyze Group Changes ---\n",
    "    # Create region map based on config\n",
    "    region_map = {country: 'Western' if country in WESTERN_COUNTRIES else 'Non-Western'\n",
    "                  for country in df_T1_full[COUNTRY_COLUMN].unique()}\n",
    "    analyze_group_changes(\n",
    "        diff_scores=diff_scores,\n",
    "        t1_aggregated_data=df_T1_full, # Pass full T1 with demographics\n",
    "        country_col=args.country_col,\n",
    "        region_map=region_map,\n",
    "        out_dir=comp_out_dir\n",
    "    )\n",
    "    \n",
    "    # --- Analyze Raw Variable Change ---\n",
    "    if args.analyze_raw:\n",
    "        analyze_raw_variable_change(\n",
    "            t1_agg_path=t1_path,\n",
    "            t2_agg_path=t2_path,\n",
    "            variable=args.raw_var,\n",
    "            country_col=args.country_col,\n",
    "            out_dir=comp_out_dir\n",
    "        )\n",
    "    \n",
    "    log_message(\"--- EFA Comparison Pipeline Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef2456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] --- Starting EFA Comparison Pipeline (T1 vs T2) ---\n",
      "Output directory ensured: output\\efa_comparison_T1_vs_T2\n",
      "Output directory ensured: output\\efa_comparison_T1_vs_T2\\T1_EFA_results\n",
      "Output directory ensured: output\\efa_comparison_T1_vs_T2\\T2_EFA_results\n",
      "[INFO] --- Loading and Preparing Data (T1 & T2) ---\n",
      "[INFO] Loaded T1: (2250, 102), T2: (2250, 104), VarInfo: (227, 4)\n",
      "[INFO] Found 20 common Binary and 77 common Ordinal variables.\n",
      "[INFO] Indices match between T1 and T2.\n",
      "[INFO] Data loading and preparation complete.\n",
      "[INFO] --- Running Factor Analysis on T1 ---\n",
      "[INFO]  Bartlett's Test: Chi2=115635.21, p=0\n",
      "[INFO]  KMO Test: Overall=0.943\n",
      "[INFO]  EFA model fitted successfully.\n",
      "[INFO]  EFA results saved to: output\\efa_comparison_T1_vs_T2\\T1_EFA_results\n",
      "[INFO] --- Running Factor Analysis on T2 ---\n",
      "[INFO]  Bartlett's Test: Chi2=inf, p=0\n",
      "[INFO]  KMO Test: Overall=0.848\n",
      "[INFO]  EFA model fitted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\CultureEval2\\.venv\\lib\\site-packages\\factor_analyzer\\factor_analyzer.py:109: RuntimeWarning: divide by zero encountered in log\n",
      "  statistic = -np.log(corr_det) * (n - 1 - (2 * p + 5) / 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  EFA results saved to: output\\efa_comparison_T1_vs_T2\\T2_EFA_results\n",
      "[INFO] --- Calculating Tucker's Congruence Coefficients ---\n",
      "[INFO] Congruence calculation complete.\n",
      "[INFO] --- Analyzing Congruence ---\n",
      "Congruence Matrix (T1 vs T2):\n",
      "          Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
      "Factor_1    -0.175     0.005     0.729     0.090     0.103\n",
      "Factor_2     0.570     0.110    -0.167     0.040     0.120\n",
      "Factor_3     0.224    -0.070     0.121    -0.273    -0.140\n",
      "Factor_4    -0.245     0.130     0.401     0.598     0.258\n",
      "Factor_5     0.011     0.030    -0.059     0.427    -0.112\n",
      "\n",
      "Best T2 Match for each T1 Factor:\n",
      "         Best_Matching_T2_Factor  Congruence_Coefficient\n",
      "Factor_1                Factor_3                   0.729\n",
      "Factor_2                Factor_1                   0.570\n",
      "Factor_3                Factor_1                   0.224\n",
      "Factor_4                Factor_4                   0.598\n",
      "Factor_5                Factor_4                   0.427\n",
      "[INFO] Diagonal congruence check: 0 factors >= 0.90, 5 factors < 0.85\n",
      "[INFO] Off-diagonal check: Max absolute value = 0.729 (Threshold < 0.30)\n",
      "[WARN] Factor structure congruence is questionable. Check coefficients.\n",
      "[INFO] --- Projecting T2 onto T1 Factors and Comparing Scores ---\n",
      "[INFO]  T2 data projected onto T1 factors.\n",
      "[INFO]  Running paired t-tests (T1 vs Projected T2)...\n",
      " Paired t-test results (T1 vs Projected T2):\n",
      "          T_Statistic  P_Value\n",
      "Factor                        \n",
      "Factor_1      48.7597      0.0\n",
      "Factor_2      12.1358      0.0\n",
      "Factor_3     -45.0082      0.0\n",
      "Factor_4      62.5731      0.0\n",
      "Factor_5     -17.5349      0.0\n",
      "[INFO]  Calculating Cohen's d for score differences...\n",
      " Cohen's d results:\n",
      "          Mean_Difference  Std_Dev_Difference  Cohen_d\n",
      "Factor                                                \n",
      "Factor_1          -0.7620              0.7412  -1.0279\n",
      "Factor_2          -0.3699              1.4458  -0.2558\n",
      "Factor_3           1.0205              1.0755   0.9489\n",
      "Factor_4          -1.1827              0.8966  -1.3192\n",
      "Factor_5           0.6909              1.8689   0.3697\n",
      "[INFO]  Score comparison complete.\n",
      "[INFO] --- Analyzing Group Changes in Factor Scores ---\n",
      "[INFO]  Calculated mean factor change by country.\n",
      "\n",
      " Mean Factor Score Change by Country (Projected T2 - T1):\n",
      "                 Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
      "B_COUNTRY_ALPHA                                                  \n",
      "AND                0.0369    0.1814    0.0049    0.0621    0.4506\n",
      "ARG               -1.0293    0.1997    0.8819   -0.2453    0.6180\n",
      "ARM               -1.2058   -0.0893    0.1890   -1.4426    0.2852\n",
      "AUS                0.3299    0.6868    0.9165   -0.7338    0.1932\n",
      "BGD               -1.1469   -1.4163    0.3345   -2.2322    0.1736\n",
      "[INFO]  Calculated mean factor change by region.\n",
      "\n",
      " Mean Factor Score Change by Region (Projected T2 - T1):\n",
      "             Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
      "region                                                       \n",
      "Non-Western   -0.8706   -0.4471    1.0531   -1.2668    0.7098\n",
      "Western        0.1391    0.2706    0.7503   -0.4850    0.5336\n",
      "[INFO] Group change analysis complete.\n",
      "[INFO] --- Analyzing Raw Change for Variable: Q165P ---\n",
      "[INFO]  Calculated mean raw change for Q165P by country.\n",
      "\n",
      " Mean Raw Score Change for Q165P by Country (T2 - T1):\n",
      "B_COUNTRY_ALPHA\n",
      "TWN   -0.9429\n",
      "CHL   -0.7941\n",
      "HKG   -0.6667\n",
      "DEU   -0.6571\n",
      "SGP   -0.6389\n",
      "        ...  \n",
      "TJK    0.0000\n",
      "TUR    0.0000\n",
      "TUN    0.0000\n",
      "UZB    0.0000\n",
      "ZWE    0.0000\n",
      "Name: Q165P_diff, Length: 66, dtype: float64\n",
      "[INFO] --- EFA Comparison Pipeline Finished ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Entry Point with Default Values (No Parser)\n",
    "# =============================================================================\n",
    "\n",
    "# Create a simple class to hold all the default values\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Input Files\n",
    "        self.t1_file = str(T1_AGGREGATED_PATH)\n",
    "        self.t2_file = str(T2_AGGREGATED_PATH)\n",
    "        self.var_file = str(VARIABLE_INFO_PATH)\n",
    "        \n",
    "        # EFA Parameters\n",
    "        self.n_factors = N_FACTORS\n",
    "        self.rotation = ROTATION\n",
    "        self.method = FACTOR_METHOD\n",
    "        self.use_smc = USE_SMC\n",
    "        \n",
    "        # Comparison Options\n",
    "        self.run_congruence = RUN_CONGRUENCE_ANALYSIS\n",
    "        self.cong_high = CONGRUENCE_THRESHOLD_HIGH\n",
    "        self.cong_low = CONGRUENCE_THRESHOLD_LOW\n",
    "        self.cong_off_diag = CONGRUENCE_OFF_DIAGONAL_MAX\n",
    "        \n",
    "        # Group Analysis Options\n",
    "        self.country_col = COUNTRY_COLUMN\n",
    "        \n",
    "        # Raw Variable Analysis Options\n",
    "        self.analyze_raw = ANALYZE_RAW_VARIABLE\n",
    "        self.raw_var = RAW_VARIABLE_TO_ANALYZE\n",
    "        \n",
    "        # Output Options\n",
    "        self.output_root = str(OUTPUT_DIR_ROOT)\n",
    "\n",
    "# Execute the main function with default arguments\n",
    "try:\n",
    "    args = Args()\n",
    "    main(args)\n",
    "except FileNotFoundError as e:\n",
    "    log_message(f\"Error: Input file not found. {e}\", \"CRITICAL\")\n",
    "    sys.exit(1)\n",
    "except (ValueError, TypeError, KeyError) as e:\n",
    "    log_message(f\"Error: Data validation or configuration issue. {e}\", \"CRITICAL\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    log_message(f\"An unexpected error occurred: {e}\", \"CRITICAL\")\n",
    "    # import traceback # Uncomment for detailed traceback\n",
    "    # traceback.print_exc()\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b98820",
   "metadata": {},
   "source": [
    "## Output with llama2:13b-chat-fp16\n",
    "```python\n",
    "Paired t-test results (T1 vs Projected T2):\n",
    "          T_Statistic  P_Value\n",
    "Factor                        \n",
    "Factor_1      42.2702      0.0\n",
    "Factor_2     -15.5940      0.0\n",
    "Factor_3      54.5968      0.0\n",
    "Factor_4      20.8409      0.0\n",
    "Factor_5      19.0582      0.0\n",
    "[INFO]  Calculating Cohen's d for score differences...\n",
    " Cohen's d results:\n",
    "          Mean_Difference  Std_Dev_Difference  Cohen_d\n",
    "Factor                                                \n",
    "Factor_1          -0.9535              1.0700  -0.8911\n",
    "Factor_2           0.4395              1.3368   0.3288\n",
    "Factor_3          -1.1983              1.0411  -1.1510\n",
    "Factor_4          -0.4261              0.9698  -0.4394\n",
    "Factor_5          -0.4183              1.0411  -0.4018\n",
    "[INFO]  Score comparison complete.\n",
    "[INFO] --- Analyzing Group Changes in Factor Scores ---\n",
    "[INFO]  Calculated mean factor change by country.\n",
    "\n",
    " Mean Factor Score Change by Country (Projected T2 - T1):\n",
    "                 Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
    "B_COUNTRY_ALPHA                                                  \n",
    "AND               -0.0062    0.5755   -2.5958    1.1834   -0.2685\n",
    "ARG               -1.1583    0.8554   -1.5766    0.7305   -0.3605\n",
    "ARM               -1.6693    0.7156   -2.1888   -0.6251   -0.2799\n",
    "AUS                0.7707    1.1320   -1.5405    0.4829   -0.3449\n",
    "BGD               -1.7075   -0.6219   -2.0154   -1.7092   -0.7582\n",
    "[INFO]  Calculated mean factor change by region.\n",
    "\n",
    " Mean Factor Score Change by Region (Projected T2 - T1):\n",
    "             Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
    "region                                                       \n",
    "Non-Western   -1.1312    0.4091   -1.1499   -0.5645   -0.4398\n",
    "Western        0.5208    0.6917   -1.5996    0.7222   -0.2398\n",
    "[INFO] Group change analysis complete.\n",
    "[INFO] --- Analyzing Raw Change for Variable: Q165P ---\n",
    "[INFO]  Calculated mean raw change for Q165P by country.\n",
    "\n",
    " Mean Raw Score Change for Q165P by Country (T2 - T1):\n",
    "B_COUNTRY_ALPHA\n",
    "CHN    0.6667\n",
    "HKG   -0.6111\n",
    "NLD    0.5556\n",
    "SGP   -0.5278\n",
    "CZE    0.5000\n",
    "        ...  \n",
    "IRQ   -0.0556\n",
    "AND   -0.0441\n",
    "NZL    0.0312\n",
    "MNG    0.0139\n",
    "PAK    0.0000\n",
    "Name: Q165P_diff, Length: 66, dtype: float64\n",
    "[INFO] --- EFA Comparison Pipeline Finished ---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91241522",
   "metadata": {},
   "source": [
    "## Output with gemma3:12b-it-fp16\n",
    "```python\n",
    "[INFO] --- Starting EFA Comparison Pipeline (T1 vs T2) ---\n",
    "Output directory ensured: output\\efa_comparison_T1_vs_T2\n",
    "Output directory ensured: output\\efa_comparison_T1_vs_T2\\T1_EFA_results\n",
    "Output directory ensured: output\\efa_comparison_T1_vs_T2\\T2_EFA_results\n",
    "[INFO] --- Loading and Preparing Data (T1 & T2) ---\n",
    "[INFO] Loaded T1: (2250, 102), T2: (2250, 104), VarInfo: (227, 4)\n",
    "[INFO] Found 20 common Binary and 77 common Ordinal variables.\n",
    "[INFO] Indices match between T1 and T2.\n",
    "[INFO] Data loading and preparation complete.\n",
    "[INFO] --- Running Factor Analysis on T1 ---\n",
    "[INFO]  Bartlett's Test: Chi2=115635.21, p=0\n",
    "[INFO]  KMO Test: Overall=0.943\n",
    "[INFO]  EFA model fitted successfully.\n",
    "[INFO]  EFA results saved to: output\\efa_comparison_T1_vs_T2\\T1_EFA_results\n",
    "[INFO] --- Running Factor Analysis on T2 ---\n",
    "[INFO]  Bartlett's Test: Chi2=688519.57, p=0\n",
    "[INFO]  KMO Test: Overall=0.978\n",
    "[INFO]  EFA model fitted successfully.\n",
    "[INFO]  EFA results saved to: output\\efa_comparison_T1_vs_T2\\T2_EFA_results\n",
    "[INFO] --- Calculating Tucker's Congruence Coefficients ---\n",
    "[INFO] Congruence calculation complete.\n",
    "[INFO] --- Analyzing Congruence ---\n",
    "Congruence Matrix (T1 vs T2):\n",
    "          Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
    "Factor_1    -0.109     0.694     0.276    -0.027    -0.097\n",
    "Factor_2     0.249    -0.046     0.076     0.112     0.515\n",
    "Factor_3     0.403     0.105    -0.403    -0.049     0.077\n",
    "Factor_4    -0.470     0.449     0.356     0.186     0.140\n",
    "Factor_5    -0.295    -0.031    -0.041     0.039     0.113\n",
    "\n",
    "Best T2 Match for each T1 Factor:\n",
    "         Best_Matching_T2_Factor  Congruence_Coefficient\n",
    "Factor_1                Factor_2                   0.694\n",
    "Factor_2                Factor_5                   0.515\n",
    "Factor_3                Factor_1                   0.403\n",
    "Factor_4                Factor_2                   0.449\n",
    "Factor_5                Factor_5                   0.113\n",
    "[INFO] Diagonal congruence check: 0 factors >= 0.90, 5 factors < 0.85\n",
    "[INFO] Off-diagonal check: Max absolute value = 0.694 (Threshold < 0.30)\n",
    "[WARN] Factor structure congruence is questionable. Check coefficients.\n",
    "[INFO] --- Projecting T2 onto T1 Factors and Comparing Scores ---\n",
    "[INFO]  T2 data projected onto T1 factors.\n",
    "[INFO]  Running paired t-tests (T1 vs Projected T2)...\n",
    " Paired t-test results (T1 vs Projected T2):\n",
    "          T_Statistic  P_Value\n",
    "Factor                        \n",
    "Factor_1      55.4551      0.0\n",
    "Factor_2      30.2494      0.0\n",
    "Factor_3       5.7951      0.0\n",
    "Factor_4      16.7415      0.0\n",
    "Factor_5     -48.2455      0.0\n",
    "[INFO]  Calculating Cohen's d for score differences...\n",
    " Cohen's d results:\n",
    "          Mean_Difference  Std_Dev_Difference  Cohen_d\n",
    "Factor                                                \n",
    "Factor_1          -1.0370              0.8870  -1.1691\n",
    "Factor_2          -0.8555              1.3415  -0.6377\n",
    "Factor_3          -0.1557              1.2747  -0.1222\n",
    "Factor_4          -0.4220              1.1957  -0.3529\n",
    "Factor_5           2.3773              2.3373   1.0171\n",
    "[INFO]  Score comparison complete.\n",
    "[INFO] --- Analyzing Group Changes in Factor Scores ---\n",
    "[INFO]  Calculated mean factor change by country.\n",
    "\n",
    " Mean Factor Score Change by Country (Projected T2 - T1):\n",
    "                 Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
    "B_COUNTRY_ALPHA                                                  \n",
    "AND               -0.0684   -0.7371   -1.5215    1.1659    2.3771\n",
    "ARG               -1.1405   -0.5816   -0.9056    0.9384    2.7462\n",
    "ARM               -1.6753   -0.5820   -1.2433   -0.6108    2.3303\n",
    "AUS                0.1844   -0.0354   -0.2090   -0.0501    1.4290\n",
    "BGD               -1.4730   -1.9649   -1.0228   -1.3853    2.0961\n",
    "[INFO]  Calculated mean factor change by region.\n",
    "\n",
    " Mean Factor Score Change by Region (Projected T2 - T1):\n",
    "             Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
    "region                                                       \n",
    "Non-Western   -1.1677   -0.9052   -0.1346   -0.5100    2.4269\n",
    "Western        0.0474   -0.4428   -0.3315    0.3077    1.9659\n",
    "[INFO] Group change analysis complete.\n",
    "[INFO] --- Analyzing Raw Change for Variable: Q165P ---\n",
    "[INFO]  Calculated mean raw change for Q165P by country.\n",
    "\n",
    " Mean Raw Score Change for Q165P by Country (T2 - T1):\n",
    "B_COUNTRY_ALPHA\n",
    "CZE    0.6667\n",
    "MAC    0.6111\n",
    "NLD    0.6111\n",
    "VNM    0.4571\n",
    "HKG   -0.4444\n",
    "        ...  \n",
    "NGA    0.0000\n",
    "PHL    0.0000\n",
    "PAK    0.0000\n",
    "TUN    0.0000\n",
    "TJK    0.0000\n",
    "Name: Q165P_diff, Length: 66, dtype: float64\n",
    "[INFO] --- EFA Comparison Pipeline Finished ---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23236ac5",
   "metadata": {},
   "source": [
    "## Output with phi4:14b-fp16\n",
    "```python\n",
    "[INFO]  EFA results saved to: output\\efa_comparison_T1_vs_T2\\T2_EFA_results\n",
    "[INFO] --- Calculating Tucker's Congruence Coefficients ---\n",
    "[INFO] Congruence calculation complete.\n",
    "[INFO] --- Analyzing Congruence ---\n",
    "Congruence Matrix (T1 vs T2):\n",
    "          Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
    "Factor_1    -0.175     0.005     0.729     0.090     0.103\n",
    "Factor_2     0.570     0.110    -0.167     0.040     0.120\n",
    "Factor_3     0.224    -0.070     0.121    -0.273    -0.140\n",
    "Factor_4    -0.245     0.130     0.401     0.598     0.258\n",
    "Factor_5     0.011     0.030    -0.059     0.427    -0.112\n",
    "\n",
    "Best T2 Match for each T1 Factor:\n",
    "         Best_Matching_T2_Factor  Congruence_Coefficient\n",
    "Factor_1                Factor_3                   0.729\n",
    "Factor_2                Factor_1                   0.570\n",
    "Factor_3                Factor_1                   0.224\n",
    "Factor_4                Factor_4                   0.598\n",
    "Factor_5                Factor_4                   0.427\n",
    "[INFO] Diagonal congruence check: 0 factors >= 0.90, 5 factors < 0.85\n",
    "[INFO] Off-diagonal check: Max absolute value = 0.729 (Threshold < 0.30)\n",
    "[WARN] Factor structure congruence is questionable. Check coefficients.\n",
    "[INFO] --- Projecting T2 onto T1 Factors and Comparing Scores ---\n",
    "[INFO]  T2 data projected onto T1 factors.\n",
    "[INFO]  Running paired t-tests (T1 vs Projected T2)...\n",
    " Paired t-test results (T1 vs Projected T2):\n",
    "          T_Statistic  P_Value\n",
    "Factor                        \n",
    "Factor_1      48.7597      0.0\n",
    "Factor_2      12.1358      0.0\n",
    "Factor_3     -45.0082      0.0\n",
    "Factor_4      62.5731      0.0\n",
    "Factor_5     -17.5349      0.0\n",
    "[INFO]  Calculating Cohen's d for score differences...\n",
    " Cohen's d results:\n",
    "          Mean_Difference  Std_Dev_Difference  Cohen_d\n",
    "Factor                                                \n",
    "Factor_1          -0.7620              0.7412  -1.0279\n",
    "Factor_2          -0.3699              1.4458  -0.2558\n",
    "Factor_3           1.0205              1.0755   0.9489\n",
    "Factor_4          -1.1827              0.8966  -1.3192\n",
    "Factor_5           0.6909              1.8689   0.3697\n",
    "[INFO]  Score comparison complete.\n",
    "[INFO] --- Analyzing Group Changes in Factor Scores ---\n",
    "[INFO]  Calculated mean factor change by country.\n",
    "\n",
    " Mean Factor Score Change by Country (Projected T2 - T1):\n",
    "                 Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
    "B_COUNTRY_ALPHA                                                  \n",
    "AND                0.0369    0.1814    0.0049    0.0621    0.4506\n",
    "ARG               -1.0293    0.1997    0.8819   -0.2453    0.6180\n",
    "ARM               -1.2058   -0.0893    0.1890   -1.4426    0.2852\n",
    "AUS                0.3299    0.6868    0.9165   -0.7338    0.1932\n",
    "BGD               -1.1469   -1.4163    0.3345   -2.2322    0.1736\n",
    "[INFO]  Calculated mean factor change by region.\n",
    "\n",
    " Mean Factor Score Change by Region (Projected T2 - T1):\n",
    "             Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
    "region                                                       \n",
    "Non-Western   -0.8706   -0.4471    1.0531   -1.2668    0.7098\n",
    "Western        0.1391    0.2706    0.7503   -0.4850    0.5336\n",
    "[INFO] Group change analysis complete.\n",
    "[INFO] --- Analyzing Raw Change for Variable: Q165P ---\n",
    "[INFO]  Calculated mean raw change for Q165P by country.\n",
    "\n",
    " Mean Raw Score Change for Q165P by Country (T2 - T1):\n",
    "B_COUNTRY_ALPHA\n",
    "TWN   -0.9429\n",
    "CHL   -0.7941\n",
    "HKG   -0.6667\n",
    "DEU   -0.6571\n",
    "SGP   -0.6389\n",
    "        ...  \n",
    "TJK    0.0000\n",
    "TUR    0.0000\n",
    "TUN    0.0000\n",
    "UZB    0.0000\n",
    "ZWE    0.0000\n",
    "Name: Q165P_diff, Length: 66, dtype: float64\n",
    "[INFO] --- EFA Comparison Pipeline Finished ---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359709cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Factor Score Differences by Model (T2 - T1):\n",
      "                            llama2-13b-chat  gemma3-12b-it  phi4-14b\n",
      "F1 - Religious-Traditional          -0.9535        -1.0370   -0.7620\n",
      "F2 - Institutional Trust             0.4395        -0.8555   -0.3699\n",
      "F3 - Democratic Values              -1.1983        -0.1557    1.0205\n",
      "F4 - Social Conservatism            -0.4261        -0.4220   -1.1827\n",
      "F5 - Openness to Diversity          -0.4183         2.3773    0.6909\n",
      "Average_Abs                          0.6871         0.9695    0.8052\n",
      "Visualizations saved to output/visualizations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# DataFrame with the mean differences from the previous outputs (Lame)\n",
    "mean_diff_comparison = pd.DataFrame({\n",
    "    'llama2-13b-chat': {\n",
    "        'F1 - Religious-Traditional': -0.9535,\n",
    "        'F2 - Institutional Trust': 0.4395,\n",
    "        'F3 - Democratic Values': -1.1983,\n",
    "        'F4 - Social Conservatism': -0.4261,\n",
    "        'F5 - Openness to Diversity': -0.4183\n",
    "    },\n",
    "    'gemma3-12b-it': {\n",
    "        'F1 - Religious-Traditional': -1.0370,\n",
    "        'F2 - Institutional Trust': -0.8555,\n",
    "        'F3 - Democratic Values': -0.1557,\n",
    "        'F4 - Social Conservatism': -0.4220,\n",
    "        'F5 - Openness to Diversity': 2.3773\n",
    "    },\n",
    "    'phi4-14b': {\n",
    "        'F1 - Religious-Traditional': -0.7620,\n",
    "        'F2 - Institutional Trust': -0.3699,\n",
    "        'F3 - Democratic Values': 1.0205,\n",
    "        'F4 - Social Conservatism': -1.1827,\n",
    "        'F5 - Openness to Diversity': 0.6909\n",
    "    }\n",
    "})\n",
    "\n",
    "# Add a row with the average absolute difference across factors\n",
    "mean_diff_comparison.loc['Average_Abs'] = mean_diff_comparison.abs().mean()\n",
    "\n",
    "# Display and save the results\n",
    "print(\"Mean Factor Score Differences by Model (T2 - T1):\")\n",
    "print(mean_diff_comparison.round(4))\n",
    "\n",
    "# Save to CSV\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "mean_diff_comparison.to_csv(f\"{output_dir}/mean_factor_score_diff_by_model.csv\")\n",
    "\n",
    "# Create visualizations directory\n",
    "viz_dir = f\"{output_dir}/visualizations\"\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "# 1. Bar chart for each factor across models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Drop the Average_Abs row for the factor comparison\n",
    "factor_data = mean_diff_comparison.drop('Average_Abs')\n",
    "factor_data.T.plot(kind='bar', width=0.8)\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.title('Mean Factor Score Differences by Model (T2 - T1)', fontsize=14)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Mean Difference (T2 - T1)', fontsize=12)\n",
    "plt.legend(title='Factor', fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{viz_dir}/factor_differences_by_model_bar.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 2. Heatmap of differences\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Create a heatmap without the Average_Abs row\n",
    "sns.heatmap(mean_diff_comparison.drop('Average_Abs'), annot=True, cmap='RdBu_r', \n",
    "            center=0, fmt='.2f', linewidths=.5)\n",
    "plt.title('Heatmap of Mean Factor Score Differences (T2 - T1)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{viz_dir}/factor_differences_heatmap.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 3. Radar/Spider chart for absolute differences\n",
    "def radar_chart(df, title):\n",
    "    # Number of variables\n",
    "    categories = list(df.index)\n",
    "    N = len(categories)\n",
    "    \n",
    "    # We are going to plot the first line of the data frame.\n",
    "    # But we need to repeat the first value to close the circular graph\n",
    "    values = df.values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variables)\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # Initialize the plot\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Draw one axis per variable + add labels\n",
    "    plt.xticks(angles[:-1], categories, size=12)\n",
    "    \n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    max_val = max(values)\n",
    "    plt.yticks([0.5, 1.0, 1.5, 2.0, 2.5], [\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\"], \n",
    "               color=\"grey\", size=10)\n",
    "    plt.ylim(0, max(2.5, max_val * 1.1))\n",
    "    \n",
    "    # Plot data\n",
    "    for i, model in enumerate(df.columns):\n",
    "        values = df[model].values.tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=model)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(title, size=15, y=1.1)\n",
    "    return fig\n",
    "\n",
    "# Create absolute differences for radar chart\n",
    "abs_diff = mean_diff_comparison.abs().drop('Average_Abs')\n",
    "radar_fig = radar_chart(abs_diff, 'Absolute Factor Score Differences by Model')\n",
    "radar_fig.savefig(f\"{viz_dir}/factor_differences_radar.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 4. Average absolute difference comparison (bar chart)\n",
    "plt.figure(figsize=(10, 6))\n",
    "avg_abs_diff = mean_diff_comparison.loc['Average_Abs'].sort_values()\n",
    "avg_abs_diff.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Average Absolute Factor Score Difference by Model', fontsize=14)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Average Absolute Difference', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{viz_dir}/average_abs_difference_by_model.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Visualizations saved to {viz_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4d596",
   "metadata": {},
   "source": [
    "This code creates four different visualizations:\n",
    "\n",
    "Bar Chart: Shows the mean difference for each factor across all models, allowing you to see which factors have the largest differences and how models compare on specific factors.\n",
    "\n",
    "Heatmap: Provides a color-coded view of all differences, making it easy to spot patterns across models and factors.\n",
    "\n",
    "Radar/Spider Chart: Displays the absolute differences in a radial format, which can help visualize which model has the smallest overall deviation across all factors.\n",
    "\n",
    "Average Absolute Difference Bar Chart: Shows which model has the lowest average absolute difference across all factors, providing a simple metric for overall model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c884089c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region visualizations saved to output/visualizations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"output\"\n",
    "viz_dir = f\"{output_dir}/visualizations\"\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "# Define the region data for each model\n",
    "# Model: llama2-13b-chat\n",
    "llama2_regions = pd.DataFrame({\n",
    "    'Non-Western': {\n",
    "        'Factor_1': -1.1312,\n",
    "        'Factor_2': 0.4091,\n",
    "        'Factor_3': -1.1499,\n",
    "        'Factor_4': -0.5645,\n",
    "        'Factor_5': -0.4398\n",
    "    },\n",
    "    'Western': {\n",
    "        'Factor_1': 0.5208,\n",
    "        'Factor_2': 0.6917,\n",
    "        'Factor_3': -1.5996,\n",
    "        'Factor_4': 0.7222,\n",
    "        'Factor_5': -0.2398\n",
    "    }\n",
    "})\n",
    "\n",
    "# Model: gemma3-12b-it\n",
    "gemma3_regions = pd.DataFrame({\n",
    "    'Non-Western': {\n",
    "        'Factor_1': -1.1677,\n",
    "        'Factor_2': -0.9052,\n",
    "        'Factor_3': -0.1346,\n",
    "        'Factor_4': -0.5100,\n",
    "        'Factor_5': 2.4269\n",
    "    },\n",
    "    'Western': {\n",
    "        'Factor_1': 0.0474,\n",
    "        'Factor_2': -0.4428,\n",
    "        'Factor_3': -0.3315,\n",
    "        'Factor_4': 0.3077,\n",
    "        'Factor_5': 1.9659\n",
    "    }\n",
    "})\n",
    "\n",
    "# Model: phi4-14b\n",
    "phi4_regions = pd.DataFrame({\n",
    "    'Non-Western': {\n",
    "        'Factor_1': -0.8706,\n",
    "        'Factor_2': -0.4471,\n",
    "        'Factor_3': 1.0531,\n",
    "        'Factor_4': -1.2668,\n",
    "        'Factor_5': 0.7098\n",
    "    },\n",
    "    'Western': {\n",
    "        'Factor_1': 0.1391,\n",
    "        'Factor_2': 0.2706,\n",
    "        'Factor_3': 0.7503,\n",
    "        'Factor_4': -0.4850,\n",
    "        'Factor_5': 0.5336\n",
    "    }\n",
    "})\n",
    "\n",
    "# Combine into a dictionary for easier access\n",
    "region_data = {\n",
    "    'llama2-13b-chat': llama2_regions,\n",
    "    'gemma3-12b-it': gemma3_regions,\n",
    "    'phi4-14b': phi4_regions\n",
    "}\n",
    "\n",
    "# 1. Grouped bar chart for each factor by region and model\n",
    "for factor in ['Factor_1', 'Factor_2', 'Factor_3', 'Factor_4', 'Factor_5']:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Extract data for this factor\n",
    "    data = {\n",
    "        'Model': [],\n",
    "        'Region': [],\n",
    "        'Mean Difference': []\n",
    "    }\n",
    "    \n",
    "    for model, df in region_data.items():\n",
    "        for region in ['Western', 'Non-Western']:\n",
    "            data['Model'].append(model)\n",
    "            data['Region'].append(region)\n",
    "            data['Mean Difference'].append(df[region][factor])\n",
    "    \n",
    "    # Convert to DataFrame for plotting\n",
    "    plot_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    sns.barplot(x='Model', y='Mean Difference', hue='Region', data=plot_df, palette='Set2')\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.title(f'Mean {factor} Score Change by Region and Model (T2 - T1)', fontsize=14)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Mean Difference (T2 - T1)', fontsize=12)\n",
    "    plt.legend(title='Region')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{viz_dir}/{factor}_by_region_model.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# 2. Heatmap for each model showing region differences\n",
    "for model, df in region_data.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df, annot=True, cmap='RdBu_r', center=0, fmt='.2f', linewidths=.5)\n",
    "    plt.title(f'Heatmap of Mean Factor Score Changes by Region for {model} (T2 - T1)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{viz_dir}/{model}_region_heatmap.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# 3. Calculate Western vs Non-Western difference for each model and factor\n",
    "west_nonwest_diff = {}\n",
    "for model, df in region_data.items():\n",
    "    west_nonwest_diff[model] = df['Western'] - df['Non-Western']\n",
    "\n",
    "west_nonwest_diff_df = pd.DataFrame(west_nonwest_diff)\n",
    "\n",
    "# Plot the Western vs Non-Western difference\n",
    "plt.figure(figsize=(12, 8))\n",
    "west_nonwest_diff_df.plot(kind='bar', width=0.8)\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.title('Difference Between Western and Non-Western Regions by Model (T2 - T1)', fontsize=14)\n",
    "plt.xlabel('Factor', fontsize=12)\n",
    "plt.ylabel('Western - Non-Western Difference', fontsize=12)\n",
    "plt.legend(title='Model', fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{viz_dir}/west_nonwest_diff_by_model.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 4. Radar chart comparing Western and Non-Western patterns for each model\n",
    "def radar_chart(df, title):\n",
    "    # Number of variables\n",
    "    categories = list(df.index)\n",
    "    N = len(categories)\n",
    "    \n",
    "    # We are going to plot the first line of the data frame.\n",
    "    # But we need to repeat the first value to close the circular graph\n",
    "    values = df.values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variables)\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # Initialize the plot\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Draw one axis per variable + add labels\n",
    "    plt.xticks(angles[:-1], categories, size=12)\n",
    "    \n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    max_val = max(abs(df.values.min()), abs(df.values.max()))\n",
    "    plt.yticks([-2, -1, 0, 1, 2], [\"-2\", \"-1\", \"0\", \"1\", \"2\"], color=\"grey\", size=10)\n",
    "    plt.ylim(-max(2, max_val * 1.1), max(2, max_val * 1.1))\n",
    "    \n",
    "    # Plot data\n",
    "    for i, region in enumerate(df.columns):\n",
    "        values = df[region].values.tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=region)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(title, size=15, y=1.1)\n",
    "    return fig\n",
    "\n",
    "for model, df in region_data.items():\n",
    "    radar_fig = radar_chart(df, f'Mean Factor Score Changes by Region for {model}')\n",
    "    radar_fig.savefig(f\"{viz_dir}/{model}_region_radar.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# 5. Combined visualization showing average absolute difference by region and model\n",
    "avg_abs_diff = {}\n",
    "for model, df in region_data.items():\n",
    "    avg_abs_diff[model] = {\n",
    "        'Western': df['Western'].abs().mean(),\n",
    "        'Non-Western': df['Non-Western'].abs().mean()\n",
    "    }\n",
    "\n",
    "avg_abs_diff_df = pd.DataFrame(avg_abs_diff).T\n",
    "avg_abs_diff_df['Overall'] = (avg_abs_diff_df['Western'] + avg_abs_diff_df['Non-Western']) / 2\n",
    "avg_abs_diff_df = avg_abs_diff_df.sort_values('Overall')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "avg_abs_diff_df[['Western', 'Non-Western', 'Overall']].plot(kind='bar', width=0.8)\n",
    "plt.title('Average Absolute Factor Score Difference by Region and Model', fontsize=14)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Average Absolute Difference', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{viz_dir}/avg_abs_diff_by_region_model.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Region visualizations saved to {viz_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e1fae",
   "metadata": {},
   "source": [
    "The code above creates several visualizations to help understand the regional differences:\n",
    "\n",
    "1. Factor-specific bar charts: For each factor, shows how the Western and Non-Western regions differ across models.\n",
    "\n",
    "2. Model-specific heatmaps: For each model, displays a heatmap of factor changes by region.\n",
    "\n",
    "3. Western vs Non-Western difference chart: Shows the gap between Western and Non-Western regions for each factor and model.\n",
    "\n",
    "4. Radar charts: For each model, displays the pattern of factor changes across regions in a radial format.\n",
    "\n",
    "5. Average absolute difference chart: Compares models based on their average absolute difference in Western and Non-Western regions.\n",
    "\n",
    "These visualizations help us answer 3 important questions:\n",
    "\n",
    "1. Which models show the largest regional differences\n",
    "2. Which factors show consistent regional patterns across models\n",
    "3. Whether certain models better capture regional variations in the ground truth data\n",
    "\n",
    "Note: Details presented in final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a6a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
