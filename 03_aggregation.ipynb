{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421818e0",
   "metadata": {},
   "source": [
    "# WVS Wave 7 Data Analysis - Aggregation Stage\n",
    "\n",
    "- This script aggregates the imputed survey data based on specified demographic profiles.\n",
    "- It calculates the mode (most frequent response) for each survey question within each unique demographic group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Imports\n",
    "# =============================================================================\n",
    "import pathlib\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='pandas') # Ignore potential pandas warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Configuration / Constants\n",
    "# =============================================================================\n",
    "DATA_DIR = pathlib.Path(\"./data\")\n",
    "\n",
    "# --- Input Data ---\n",
    "# IMPUTED_DATA_PATH = DATA_DIR / \"wvs_wave7_imputed_final.csv\" # From MICE with custom estimators\n",
    "IMPUTED_DATA_PATH = DATA_DIR / \"wvs_wave7_imputed.csv\" # From default IterativeImputer (Chosen based on performance)\n",
    "\n",
    "# --- Output Data ---\n",
    "AGGREGATED_OUTPUT_CSV_PATH = DATA_DIR / \"new_mode_wvs_wave7_aggregated_by_demographics.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregation Parameters ---\n",
    "# Define demographic variables used for grouping during aggregation\n",
    "# Should match variables used for grouping in the imputation stage ideally\n",
    "AGGREGATION_DEMOGRAPHIC_VARS = [\n",
    "    'B_COUNTRY_ALPHA', # country\n",
    "    'H_URBRURAL',      # urban / rural\n",
    "    # 'Q273',          # marital status\n",
    "    'Q260',            # sex\n",
    "    'X003R2',          # age (3-cat)\n",
    "    'Q275R',           # education (3 groups)\n",
    "    # 'Q279',          # employment\n",
    "]\n",
    "\n",
    "# Aggregation function to use ('mode', 'median')\n",
    "AGGREGATION_METHOD = 'mode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Function Definitions\n",
    "# =============================================================================\n",
    "\n",
    "def load_imputed_data(filepath: pathlib.Path) -> pd.DataFrame:\n",
    "    \"\"\"Loads the imputed data CSV file.\"\"\"\n",
    "    print(f\"Loading imputed data from: {filepath}\")\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Error: Imputed data file not found at '{filepath}'. \"\n",
    "            \"Please ensure the imputation script ran successfully and saved the file.\"\n",
    "        )\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Loaded imputed data shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading imputed data file '{filepath}': {e}\")\n",
    "\n",
    "    # Validate that required demographic columns exist\n",
    "    missing_req_cols = [col for col in AGGREGATION_DEMOGRAPHIC_VARS if col not in df.columns]\n",
    "    if missing_req_cols:\n",
    "         raise ValueError(f\"Error: Required demographic columns for aggregation missing from imputed data: {missing_req_cols}\")\n",
    "    return df\n",
    "\n",
    "def identify_columns(df: pd.DataFrame, demographic_vars: list[str]) -> tuple[list[str], list[str]]:\n",
    "    \"\"\"Separates demographic variables from survey question columns.\"\"\"\n",
    "    print(\"Identifying demographic and survey columns...\")\n",
    "    all_columns = df.columns.tolist()\n",
    "    # To ensure demographic vars provided are actually in the dataframe columns\n",
    "    valid_demographic_vars = [var for var in demographic_vars if var in all_columns]\n",
    "    if len(valid_demographic_vars) < len(demographic_vars):\n",
    "        print(f\"Warning: Not all specified demographic vars found in data. Using: {valid_demographic_vars}\")\n",
    "\n",
    "    survey_question_cols = [\n",
    "        col for col in all_columns if col not in valid_demographic_vars\n",
    "    ]\n",
    "    print(f\"Found {len(valid_demographic_vars)} demographic variables.\")\n",
    "    print(f\"Found {len(survey_question_cols)} survey question columns.\")\n",
    "\n",
    "    if not survey_question_cols:\n",
    "        raise ValueError(\"Error: No survey question columns identified. Check demographic variable list.\")\n",
    "\n",
    "    return valid_demographic_vars, survey_question_cols\n",
    "\n",
    "def aggregate_data_by_mode(df: pd.DataFrame, group_by_cols: list[str], aggregate_cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates survey responses by demographic groups using the mode (most frequent value).\n",
    "    Handles potential multiple modes by taking the first one.\n",
    "    \"\"\"\n",
    "    print(f\"Aggregating {len(aggregate_cols)} survey columns by {len(group_by_cols)} demographic variables using mode...\")\n",
    "    if not group_by_cols:\n",
    "         raise ValueError(\"Error: No columns specified for grouping.\")\n",
    "    if not aggregate_cols:\n",
    "         raise ValueError(\"Error: No columns specified for aggregation.\")\n",
    "\n",
    "    # Define the mode function\n",
    "    def get_first_mode(x):\n",
    "        modes = x.mode()\n",
    "        # Return first mode if modes exist otherwise return NaN\n",
    "        return modes.iloc[0] if not modes.empty else np.nan\n",
    "\n",
    "    try:\n",
    "        # Group by the demographic variables\n",
    "        grouped = df.groupby(group_by_cols, observed=False, dropna=True) # Handle NaNs in grouping keys if needed\n",
    "\n",
    "        # Apply the mode aggregation to the survey columns\n",
    "        aggregated_data = grouped[aggregate_cols].apply(lambda x: x.apply(get_first_mode, axis=0), include_groups=False).reset_index()\n",
    "        # Alternative is agg (lambda worked so ignoring this for now)\n",
    "        # aggregated_data = grouped[aggregate_cols].agg(get_first_mode).reset_index()\n",
    "\n",
    "        print(f\"Aggregation complete. Reduced dataset from {len(df):,} rows to {len(aggregated_data):,} rows (unique demographic profiles).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during aggregation: {e}\")\n",
    "        raise RuntimeError(f\"Aggregation failed: {e}\")\n",
    "\n",
    "    return aggregated_data\n",
    "\n",
    "def save_aggregated_data(df: pd.DataFrame, filepath: pathlib.Path):\n",
    "    \"\"\"Saves the aggregated DataFrame to a CSV file.\"\"\"\n",
    "    print(f\"\\nSaving aggregated data to: {filepath}\")\n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"Successfully saved {filepath} ({df.shape[0]:,} rows Ã— {df.shape[1]} cols)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving aggregated data to {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e0b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Main Execution Workflow\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the data aggregation pipeline.\"\"\"\n",
    "    print(\"--- Starting WVS Data Aggregation Pipeline ---\")\n",
    "\n",
    "    try:\n",
    "        # 1. Load Imputed Data\n",
    "        df_imputed = load_imputed_data(IMPUTED_DATA_PATH)\n",
    "\n",
    "        # # After load_imputed_data:\n",
    "        # Dealt with this mess already but going to keep it for reference\n",
    "        # columns_to_drop = ['H_URBRURAL'] # Add any other columns to drop entirely\n",
    "        # df_imputed = df_imputed.drop(columns=[col for col in columns_to_drop if col in df_imputed.columns])\n",
    "\n",
    "        # 2. Identify Columns for Grouping and Aggregation\n",
    "        demographic_cols, survey_cols = identify_columns(df_imputed, AGGREGATION_DEMOGRAPHIC_VARS)\n",
    "\n",
    "        # 3. Aggregate Data (Using Mode)\n",
    "        if AGGREGATION_METHOD == 'mode':\n",
    "            df_aggregated = aggregate_data_by_mode(df_imputed, demographic_cols, survey_cols)\n",
    "\n",
    "        # Separate experiment done next\n",
    "        # elif AGGREGATION_METHOD == 'median':\n",
    "        #     # Implement median aggregation if needed\n",
    "        #     pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported AGGREGATION_METHOD: {AGGREGATION_METHOD}\")\n",
    "\n",
    "        # 4. Display Head of Aggregated Data\n",
    "        print(\"\\nFirst 5 rows of aggregated data:\")\n",
    "        print(df_aggregated.head())\n",
    "\n",
    "        # 5. Save Aggregated Data\n",
    "        save_aggregated_data(df_aggregated, AGGREGATED_OUTPUT_CSV_PATH)\n",
    "\n",
    "        print(\"\\n--- WVS Data Aggregation Pipeline Finished ---\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nCritical Error: Input file not found.\")\n",
    "        print(e)\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nCritical Error: Configuration or Data invalid.\")\n",
    "        print(e)\n",
    "    except RuntimeError as e:\n",
    "         print(f\"\\nCritical Error: Aggregation process failed.\")\n",
    "         print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during the pipeline execution:\")\n",
    "        print(e)\n",
    "        # import traceback\n",
    "        # traceback.print_exc() # Uncomment for detailed error traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04099f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting WVS Data Aggregation Pipeline ---\n",
      "Loading imputed data from: data\\wvs_wave7_imputed.csv\n",
      "Loaded imputed data shape: (97220, 102)\n",
      "Identifying demographic and survey columns...\n",
      "Found 5 demographic variables.\n",
      "Found 97 survey question columns.\n",
      "Aggregating 97 survey columns by 5 demographic variables using mode...\n",
      "Aggregation complete. Reduced dataset from 97,220 rows to 2,250 rows (unique demographic profiles).\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "  B_COUNTRY_ALPHA  H_URBRURAL  Q260  X003R2  Q275R  Q106  Q107  Q108  Q109  \\\n",
      "0             AND         1.0   1.0     1.0    1.0  10.0   5.0  10.0   1.0   \n",
      "1             AND         1.0   1.0     1.0    2.0   8.0   5.0   2.0   2.0   \n",
      "2             AND         1.0   1.0     1.0    3.0   8.0   5.0   2.0   1.0   \n",
      "3             AND         1.0   1.0     2.0    1.0  10.0   5.0   5.0   2.0   \n",
      "4             AND         1.0   1.0     2.0    2.0   7.0   5.0   5.0   1.0   \n",
      "\n",
      "   Q110  ...  Q69P  Q6P  Q70P  Q71P  Q72P  Q73P  Q77P  Q8P  Q90  Q9P  \n",
      "0  10.0  ...   3.0  1.0   2.0   1.0   1.0   2.0   1.0  0.0  1.0  0.0  \n",
      "1   3.0  ...   3.0  1.0   3.0   2.0   2.0   2.0   3.0  0.0  2.0  0.0  \n",
      "2   5.0  ...   3.0  1.0   3.0   3.0   2.0   3.0   3.0  0.0  2.0  0.0  \n",
      "3   3.0  ...   3.0  1.0   3.0   2.0   1.0   2.0   3.0  0.0  5.0  1.0  \n",
      "4   4.0  ...   3.0  1.0   3.0   3.0   2.0   2.0   2.0  0.0  2.0  0.0  \n",
      "\n",
      "[5 rows x 102 columns]\n",
      "\n",
      "Saving aggregated data to: data\\new_mode_wvs_wave7_aggregated_by_demographics.csv\n",
      "Successfully saved data\\new_mode_wvs_wave7_aggregated_by_demographics.csv (2,250 rows Ã— 102 cols)\n",
      "\n",
      "--- WVS Data Aggregation Pipeline Finished ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60e93d",
   "metadata": {},
   "source": [
    "```python\n",
    "--- Starting WVS Data Aggregation Pipeline ---\n",
    "Loading imputed data from: data\\wvs_wave7_imputed.csv\n",
    "Loaded imputed data shape: (97220, 102)\n",
    "Identifying demographic and survey columns...\n",
    "Found 5 demographic variables.\n",
    "Found 97 survey question columns.\n",
    "Aggregating 97 survey columns by 5 demographic variables using mode...\n",
    "Aggregation complete. Reduced dataset from 97,220 rows to 2,250 rows (unique demographic profiles).\n",
    "\n",
    "First 5 rows of aggregated data:\n",
    "  B_COUNTRY_ALPHA  H_URBRURAL  Q260  X003R2  Q275R  Q106  Q107  Q108  Q109  \\\n",
    "0             AND         1.0   1.0     1.0    1.0  10.0   5.0  10.0   1.0   \n",
    "1             AND         1.0   1.0     1.0    2.0   8.0   5.0   2.0   2.0   \n",
    "2             AND         1.0   1.0     1.0    3.0   8.0   5.0   2.0   1.0   \n",
    "3             AND         1.0   1.0     2.0    1.0  10.0   5.0   5.0   2.0   \n",
    "4             AND         1.0   1.0     2.0    2.0   7.0   5.0   5.0   1.0   \n",
    "\n",
    "   Q110  ...  Q69P  Q6P  Q70P  Q71P  Q72P  Q73P  Q77P  Q8P  Q90  Q9P  \n",
    "0  10.0  ...   3.0  1.0   2.0   1.0   1.0   2.0   1.0  0.0  1.0  0.0  \n",
    "1   3.0  ...   3.0  1.0   3.0   2.0   2.0   2.0   3.0  0.0  2.0  0.0  \n",
    "2   5.0  ...   3.0  1.0   3.0   3.0   2.0   3.0   3.0  0.0  2.0  0.0  \n",
    "3   3.0  ...   3.0  1.0   3.0   2.0   1.0   2.0   3.0  0.0  5.0  1.0  \n",
    "4   4.0  ...   3.0  1.0   3.0   3.0   2.0   2.0   2.0  0.0  2.0  0.0  \n",
    "\n",
    "[5 rows x 102 columns]\n",
    "\n",
    "Saving aggregated data to: data\\new_mode_wvs_wave7_aggregated_by_demographics.csv\n",
    "Successfully saved data\\new_mode_wvs_wave7_aggregated_by_demographics.csv (2,250 rows Ã— 102 cols)\n",
    "\n",
    "--- WVS Data Aggregation Pipeline Finished ---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff1cf4",
   "metadata": {},
   "source": [
    "## Using Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Imports\n",
    "# =============================================================================\n",
    "import pathlib\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='pandas') # Ignore potential pandas warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Configuration / Constants\n",
    "# =============================================================================\n",
    "DATA_DIR = pathlib.Path(\"./data\")\n",
    "\n",
    "# --- Input Data ---\n",
    "# IMPUTED_DATA_PATH = DATA_DIR / \"wvs_wave7_imputed_final.csv\" # From MICE with custom estimators\n",
    "IMPUTED_DATA_PATH = DATA_DIR / \"wvs_wave7_imputed.csv\" # From default IterativeImputer\n",
    "\n",
    "# --- Output Data ---\n",
    "# Update output filename to reflect median usage\n",
    "AGGREGATED_OUTPUT_CSV_PATH = DATA_DIR / \"new_median_wvs_wave7_aggregated_by_demographics.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregation Parameters ---\n",
    "# Define demographic variables used for grouping during aggregation\n",
    "# Should match variables used for grouping in the imputation stage ideally\n",
    "AGGREGATION_DEMOGRAPHIC_VARS = [ # If removing any variables, ensure they are also updated in `columns_to_drop`\n",
    "    'B_COUNTRY_ALPHA', # country\n",
    "    'H_URBRURAL',      # urban / rural\n",
    "    # 'Q273',          # marital status\n",
    "    'Q260',            # sex\n",
    "    'X003R2',          # age (3-cat)\n",
    "    'Q275R',           # education (3 groups)\n",
    "    # 'Q279',          # employment\n",
    "]\n",
    "\n",
    "# Aggregation function to use ('mode', 'median')\n",
    "AGGREGATION_METHOD = 'median'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2344c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Function Definitions\n",
    "# =============================================================================\n",
    "\n",
    "def load_imputed_data(filepath: pathlib.Path) -> pd.DataFrame:\n",
    "    \"\"\"Loads the imputed data CSV file.\"\"\"\n",
    "    print(f\"Loading imputed data from: {filepath}\")\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Error: Imputed data file not found at '{filepath}'. \"\n",
    "            \"Please ensure the imputation script ran successfully and saved the file.\"\n",
    "        )\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Loaded imputed data shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading imputed data file '{filepath}': {e}\")\n",
    "\n",
    "    # Validate that required demographic columns exist (after potential dropping in main)\n",
    "    # We will validate against the final list of demo vars used in identify_columns\n",
    "    missing_req_cols = [col for col in AGGREGATION_DEMOGRAPHIC_VARS if col not in df.columns]\n",
    "    if missing_req_cols:\n",
    "         raise ValueError(f\"Error: Required demographic columns for aggregation missing from imputed data: {missing_req_cols}\")\n",
    "    return df\n",
    "\n",
    "def identify_columns(df: pd.DataFrame, demographic_vars: list[str]) -> tuple[list[str], list[str]]:\n",
    "    \"\"\"Separates demographic variables from survey question columns.\"\"\"\n",
    "    print(\"Identifying demographic and survey columns...\")\n",
    "    all_columns = df.columns.tolist()\n",
    "\n",
    "    # Ensure demographic vars provided are actually in the dataframe columns *after* any drops\n",
    "    valid_demographic_vars = [var for var in demographic_vars if var in all_columns]\n",
    "    if len(valid_demographic_vars) < len(demographic_vars):\n",
    "        # This check happens *after* potential drops in main(), so it's okay if some are missing now\n",
    "        print(f\"Warning: Using these demographic vars found in data for grouping: {valid_demographic_vars}\")\n",
    "        # Update the demographic_vars list to only include those present\n",
    "        demographic_vars[:] = valid_demographic_vars\n",
    "\n",
    "\n",
    "    survey_question_cols = [\n",
    "        col for col in all_columns if col not in valid_demographic_vars\n",
    "    ]\n",
    "    print(f\"Found {len(valid_demographic_vars)} demographic variables for grouping.\")\n",
    "    print(f\"Found {len(survey_question_cols)} survey question columns for aggregation.\")\n",
    "\n",
    "    if not survey_question_cols:\n",
    "        raise ValueError(\"Error: No survey question columns identified. Check demographic variable list and dropped columns.\")\n",
    "    if not valid_demographic_vars:\n",
    "        raise ValueError(\"Error: No demographic columns identified for grouping. Check demographic variable list and dropped columns.\")\n",
    "\n",
    "\n",
    "    return valid_demographic_vars, survey_question_cols\n",
    "\n",
    "# --- NEW FUNCTION FOR MEDIAN ---\n",
    "def aggregate_data_by_median(df: pd.DataFrame, group_by_cols: list[str], aggregate_cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates survey responses by demographic groups using the median value.\n",
    "    Assumes aggregate_cols contain numeric or ordinal data where median is meaningful.\n",
    "    \"\"\"\n",
    "    print(f\"Aggregating {len(aggregate_cols)} survey columns by {len(group_by_cols)} demographic variables using median...\")\n",
    "    if not group_by_cols:\n",
    "         raise ValueError(\"Error: No columns specified for grouping.\")\n",
    "    if not aggregate_cols:\n",
    "         raise ValueError(\"Error: No columns specified for aggregation.\")\n",
    "\n",
    "    # To ensure aggregation columns are numeric (median)\n",
    "    non_numeric_cols = df[aggregate_cols].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    if non_numeric_cols:\n",
    "        print(f\"Warning: The following columns intended for median aggregation are non-numeric and will likely result in NaNs: {non_numeric_cols}\")\n",
    "        # Previously used for taking care of errors:\n",
    "        # 1. Raise an error: raise TypeError(f\"Columns for median aggregation must be numeric. Found non-numeric: {non_numeric_cols}\")\n",
    "        # 2. Try to convert them: # df[non_numeric_cols] = df[non_numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "        # 3. Exclude them: # aggregate_cols = [col for col in aggregate_cols if col not in non_numeric_cols]\n",
    "\n",
    "    try:\n",
    "        # Group by the demographic variables\n",
    "        grouped = df.groupby(group_by_cols, observed=False, dropna=True) # Handle NaNs in grouping keys if needed\n",
    "\n",
    "        # Calculate the median for the specified survey columns within each group        \n",
    "        aggregated_data = grouped[aggregate_cols].median().reset_index() # .median() automatically handles NaNs within each group's column by default (skips them)\n",
    "\n",
    "        print(f\"Aggregation complete. Reduced dataset from {len(df):,} rows to {len(aggregated_data):,} rows (unique demographic profiles).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during median aggregation: {e}\")\n",
    "        raise RuntimeError(f\"Median aggregation failed: {e}\")\n",
    "\n",
    "    return aggregated_data\n",
    "\n",
    "def save_aggregated_data(df: pd.DataFrame, filepath: pathlib.Path):\n",
    "    \"\"\"Saves the aggregated DataFrame to a CSV file.\"\"\"\n",
    "    print(f\"\\nSaving aggregated data to: {filepath}\")\n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"Successfully saved {filepath} ({df.shape[0]:,} rows Ã— {df.shape[1]} cols)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving aggregated data to {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdedfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Main Execution Workflow\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the data aggregation pipeline.\"\"\"\n",
    "    print(f\"--- Starting WVS Data Aggregation Pipeline (Method: {AGGREGATION_METHOD}) ---\")\n",
    "\n",
    "    try:\n",
    "        # 1. Load Imputed Data\n",
    "        df_imputed = load_imputed_data(IMPUTED_DATA_PATH)\n",
    "\n",
    "        # Make a copy of the configured demographic vars to pass to identify_columns\n",
    "        # This list might be modified *within* identify_columns if some vars aren't found\n",
    "        current_demographic_vars = AGGREGATION_DEMOGRAPHIC_VARS.copy()\n",
    "\n",
    "        # 2. Identify Columns for Grouping and Aggregation\n",
    "        # To validate that the demo vars actually exist *after* dropping\n",
    "        demographic_cols, survey_cols = identify_columns(df_imputed, current_demographic_vars)\n",
    "\n",
    "\n",
    "        # 3. Aggregate Data (Using configured method)\n",
    "        if AGGREGATION_METHOD == 'median':\n",
    "            df_aggregated = aggregate_data_by_median(df_imputed, demographic_cols, survey_cols)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported AGGREGATION_METHOD: {AGGREGATION_METHOD}. Choose 'median'.\") # Or add back 'mode'/'mean' support\n",
    "\n",
    "        # 4. Display Head of Aggregated Data (Optional)\n",
    "        print(\"\\nFirst 5 rows of aggregated data:\")\n",
    "        print(df_aggregated.head())\n",
    "\n",
    "        # 5. Save Aggregated Data\n",
    "        save_aggregated_data(df_aggregated, AGGREGATED_OUTPUT_CSV_PATH)\n",
    "\n",
    "        print(f\"\\n--- WVS Data Aggregation Pipeline (Method: {AGGREGATION_METHOD}) Finished ---\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nCritical Error: Input file not found.\")\n",
    "        print(e)\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nCritical Error: Configuration or Data invalid.\")\n",
    "        print(e)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\nCritical Error: Aggregation process failed.\")\n",
    "        print(e)\n",
    "    except NotImplementedError as e:\n",
    "         print(f\"\\nCritical Error: Functionality not implemented.\")\n",
    "         print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during the pipeline execution:\")\n",
    "        print(e)\n",
    "        # import traceback\n",
    "        # traceback.print_exc() # Uncomment for detailed error traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06158b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting WVS Data Aggregation Pipeline (Method: median) ---\n",
      "Loading imputed data from: data\\wvs_wave7_imputed.csv\n",
      "Loaded imputed data shape: (97220, 102)\n",
      "Identifying demographic and survey columns...\n",
      "Found 5 demographic variables for grouping.\n",
      "Found 97 survey question columns for aggregation.\n",
      "Aggregating 97 survey columns by 5 demographic variables using median...\n",
      "Aggregation complete. Reduced dataset from 97,220 rows to 2,250 rows (unique demographic profiles).\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "  B_COUNTRY_ALPHA  H_URBRURAL  Q260  X003R2  Q275R  Q106  Q107  Q108  Q109  \\\n",
      "0             AND         1.0   1.0     1.0    1.0   8.0   5.0   7.0   1.5   \n",
      "1             AND         1.0   1.0     1.0    2.0   7.0   5.0   3.0   2.0   \n",
      "2             AND         1.0   1.0     1.0    3.0   6.0   5.0   4.0   2.0   \n",
      "3             AND         1.0   1.0     2.0    1.0   5.0   5.0   5.0   2.0   \n",
      "4             AND         1.0   1.0     2.0    2.0   7.0   5.0   5.0   2.0   \n",
      "\n",
      "   Q110  ...  Q69P  Q6P  Q70P  Q71P  Q72P  Q73P  Q77P  Q8P  Q90  Q9P  \n",
      "0   6.0  ...   3.0  1.5   2.0   1.5   1.0   2.0   2.0  0.0  5.0  0.5  \n",
      "1   4.0  ...   3.0  1.0   3.0   2.0   2.0   2.0   3.0  0.0  5.0  0.0  \n",
      "2   5.0  ...   3.0  1.0   3.0   3.0   2.0   2.0   2.0  0.0  4.0  0.0  \n",
      "3   4.0  ...   3.0  2.0   3.0   2.0   2.0   2.0   3.0  0.0  4.0  1.0  \n",
      "4   4.0  ...   3.0  2.0   3.0   2.0   2.0   2.0   2.0  0.0  4.0  0.0  \n",
      "\n",
      "[5 rows x 102 columns]\n",
      "\n",
      "Saving aggregated data to: data\\new_median_wvs_wave7_aggregated_by_demographics.csv\n",
      "Successfully saved data\\new_median_wvs_wave7_aggregated_by_demographics.csv (2,250 rows Ã— 102 cols)\n",
      "\n",
      "--- WVS Data Aggregation Pipeline (Method: median) Finished ---\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7abbf",
   "metadata": {},
   "source": [
    "```python\n",
    "--- Starting WVS Data Aggregation Pipeline (Method: median) ---\n",
    "Loading imputed data from: data\\wvs_wave7_imputed.csv\n",
    "Loaded imputed data shape: (97220, 102)\n",
    "Identifying demographic and survey columns...\n",
    "Found 5 demographic variables for grouping.\n",
    "Found 97 survey question columns for aggregation.\n",
    "Aggregating 97 survey columns by 5 demographic variables using median...\n",
    "Aggregation complete. Reduced dataset from 97,220 rows to 2,250 rows (unique demographic profiles).\n",
    "\n",
    "First 5 rows of aggregated data:\n",
    "  B_COUNTRY_ALPHA  H_URBRURAL  Q260  X003R2  Q275R  Q106  Q107  Q108  Q109  \\\n",
    "0             AND         1.0   1.0     1.0    1.0   8.0   5.0   7.0   1.5   \n",
    "1             AND         1.0   1.0     1.0    2.0   7.0   5.0   3.0   2.0   \n",
    "2             AND         1.0   1.0     1.0    3.0   6.0   5.0   4.0   2.0   \n",
    "3             AND         1.0   1.0     2.0    1.0   5.0   5.0   5.0   2.0   \n",
    "4             AND         1.0   1.0     2.0    2.0   7.0   5.0   5.0   2.0   \n",
    "\n",
    "   Q110  ...  Q69P  Q6P  Q70P  Q71P  Q72P  Q73P  Q77P  Q8P  Q90  Q9P  \n",
    "0   6.0  ...   3.0  1.5   2.0   1.5   1.0   2.0   2.0  0.0  5.0  0.5  \n",
    "1   4.0  ...   3.0  1.0   3.0   2.0   2.0   2.0   3.0  0.0  5.0  0.0  \n",
    "2   5.0  ...   3.0  1.0   3.0   3.0   2.0   2.0   2.0  0.0  4.0  0.0  \n",
    "3   4.0  ...   3.0  2.0   3.0   2.0   2.0   2.0   3.0  0.0  4.0  1.0  \n",
    "4   4.0  ...   3.0  2.0   3.0   2.0   2.0   2.0   2.0  0.0  4.0  0.0  \n",
    "\n",
    "[5 rows x 102 columns]\n",
    "\n",
    "Saving aggregated data to: data\\new_median_wvs_wave7_aggregated_by_demographics.csv\n",
    "Successfully saved data\\new_median_wvs_wave7_aggregated_by_demographics.csv (2,250 rows Ã— 102 cols)\n",
    "\n",
    "--- WVS Data Aggregation Pipeline (Method: median) Finished ---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7804e819",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
